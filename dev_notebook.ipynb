{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import bert\n",
    "# from bert import run_classifier\n",
    "# from bert import optimization\n",
    "#from bert import tokenization\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "from tensorflow.metrics import accuracy\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing the preprocessing module\n",
    "\n",
    "from preprocessing.preprocessing import convert_example, FeatureWriter\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                                  tokenization_info[\"do_lower_case\"]])\n",
    "\n",
    "    return bert.tokenization.FullTokenizer(\n",
    "        vocab_file=vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "token = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:/data/nq/natural_questions/v1.0/sample_train/train/nq-train-sample.jsonl:0\n",
      "INFO:tensorflow:(276, 279)\n",
      "INFO:tensorflow:(148, 151)\n",
      "INFO:tensorflow:(20, 23)\n",
      "INFO:tensorflow:3 examples found\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import re\n",
    "_train_file = '/data/nq/natural_questions/v1.0/sample_train/train/nq-train-sample.jsonl'\n",
    "_train_file_out = re.sub(\".jsonl\", \".tf_record\", _train_file)\n",
    "train_writer = FeatureWriter(\n",
    "    filename=_train_file_out,\n",
    "    is_training=True)\n",
    "with jsonlines.open(_train_file) as reader:\n",
    "    features, examples = [], []\n",
    "    for i, example in enumerate(reader):\n",
    "        if i % 1e3 == 0: tf.logging.info(\"{}:{}\".format(_train_file, i))\n",
    "        examples.append(example)\n",
    "        dt = convert_example(example,\n",
    "                             tokenizer=token,\n",
    "                             is_training=True,\n",
    "                             max_seq_length=384,\n",
    "                             doc_stride=128,\n",
    "                             max_query_length=64,\n",
    "                             train_writer=train_writer.process_feature)\n",
    "        train_writer.close()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples), len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yes / no\n",
    "#[ex['annotations'][0]['yes_no_answer'] for ex in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2975172535563055798"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = features[10]\n",
    "feature.example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(i):\n",
    "    feature = features[i]\n",
    "    example_id = feature.example_id\n",
    "    example = [x for x in examples if x['example_id'] == example_id][0]\n",
    "    return feature, example  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotations(example):\n",
    "    \"\"\"\n",
    "    if short, else long\n",
    "    \"\"\"\n",
    "    annotation = example['annotations'][0]\n",
    "    end_byte_ix, start_byte_ix = None, None\n",
    "    start_token, end_token = None, None\n",
    "    if annotation['short_answers']:\n",
    "        end_byte_ix = annotation['short_answers'][0]['end_byte']\n",
    "        start_token = annotation['short_answers'][0]['start_token']\n",
    "        end_token = annotation['short_answers'][0]['end_token']\n",
    "        start_byte_ix = annotation['short_answers'][0]['start_byte']\n",
    "    else:\n",
    "        end_byte_ix = annotation['long_answer']['end_byte']\n",
    "        start_byte_ix = annotation['long_answer']['start_byte']\n",
    "        start_token = annotation['long_answer']['start_token']\n",
    "        end_token = annotation['long_answer']['end_token']\n",
    "    return {'end_byte_ix': end_byte_ix, \n",
    "            'start_byte_ix': start_byte_ix,\n",
    "            'start_token': start_token,\n",
    "            'end_token': end_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate(i):\n",
    "    # get the feature and example the feature is derived from.\n",
    "    feature, example = test(i)\n",
    "    # get the ground truth annotations.\n",
    "    gt = get_annotations(example)\n",
    "    # get start byte and end bytes for targets.\n",
    "    if feature.targets[0] == 0:\n",
    "        return (i, True)\n",
    "    start_bytes = feature.start_bytes[feature.targets[0]]\n",
    "    end_bytes = feature.end_bytes[feature.targets[1]]\n",
    "    if start_bytes == gt['start_byte_ix'] and end_bytes == gt['end_byte_ix']:\n",
    "        return (i,True)\n",
    "    else:\n",
    "        return (i, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'end_byte_ix': 96731, 'start_byte_ix': 96715, 'start_token': 3521, 'end_token': 3525}\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, True)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature, example = test(1)\n",
    "gt = get_annotations(example)\n",
    "print(gt)\n",
    "start_bytes = feature.start_bytes[feature.targets[0]]\n",
    "end_bytes = feature.end_bytes[feature.targets[1]]\n",
    "feature.targets, start_bytes, end_bytes\n",
    "_validate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = []\n",
    "for i in range(len(features)):\n",
    "    _assertion =  _validate(i)\n",
    "    if not _assertion[1]:\n",
    "        ix.append(i)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth\n",
      "{'end_byte_ix': 55798, 'start_byte_ix': 55137, 'start_token': 893, 'end_token': 1001}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55140, 55794)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assertion fails\n",
    "feature, example = test(ix[2])\n",
    "gt = get_annotations(example)\n",
    "print('ground truth')\n",
    "print(gt)\n",
    "start_bytes = feature.start_bytes[feature.targets[0]]\n",
    "end_bytes = feature.end_bytes[feature.targets[1]]\n",
    "start_bytes, end_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'annotation_id': 13306123758205215060,\n",
       "  'long_answer': {'candidate_index': 32,\n",
       "   'end_byte': 55798,\n",
       "   'end_token': 1001,\n",
       "   'start_byte': 55137,\n",
       "   'start_token': 893},\n",
       "  'short_answers': [],\n",
       "  'yes_no_answer': 'NONE'}]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ann = get_annotations(example)\n",
    "#[t for t in example['document_tokens'] if t['start_byte'] >= ann['start_byte_ix'] and t['end_byte'] <= ann['end_byte_ix']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_nq import input_fn_builder\n",
    "seq_length=384\n",
    "\n",
    "name_to_features = {\n",
    "  \"input_ids\": tf.FixedLenFeature([], tf.int64),\n",
    "  \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "  \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "  \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "  \"start_bytes\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "  \"end_bytes\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "}\n",
    "name_to_features[\"start_positions\"] = tf.FixedLenFeature([], tf.int64)\n",
    "name_to_features[\"end_positions\"] = tf.FixedLenFeature([], tf.int64)\n",
    "\n",
    "def _decode_record(record):\n",
    "  \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "  example = tf.parse_single_example(record, name_to_features)\n",
    "  return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import run_nq\n",
    "#bert_data_dir = '/Users/deniz/natural_questions/data/'\n",
    "#_train_path = '/Users/deniz/natural_questions/data/v1.0_sample_nq-train-sample.jsonl'\n",
    "_train_path = '/data/nq/natural_questions/v1.0/sample_train/'\n",
    "#_dev_path = os.path.join(bert_data_dir, 'dev')\n",
    "_train_path = os.path.join(_train_path, 'train')\n",
    "train_files = [os.path.join(_train_path, _file) for _file in os.listdir(_train_path) if _file.endswith(\".tf_record\")]\n",
    "\n",
    "train_input_fn = input_fn_builder(\n",
    "  input_files=train_files,\n",
    "  seq_length=384,\n",
    "  is_training=True,\n",
    "  mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['end_bytes', 'end_positions', 'input_ids', 'input_mask', 'segment_ids', 'start_bytes', 'start_positions'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "dt = tf.data.TFRecordDataset(train_files)\n",
    "dt = dt.map(_decode_record, num_parallel_calls=10)\n",
    "dt = dt.shuffle(buffer_size=100)\n",
    "dt = dt.batch(32)\n",
    "it = dt.make_one_shot_iterator()\n",
    "a = it.get_next()\n",
    "a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=168, shape=(3,), dtype=int64, numpy=array([148,  20, 276])>,\n",
       " <tf.Tensor: id=163, shape=(3,), dtype=int64, numpy=array([151,  23, 279])>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['start_positions'], a['end_positions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context\n",
    "a = tf.constant([[2,10],[3,20],[4,50]])\n",
    "a = tf.constant([[0.2,0.5,0.3]])\n",
    "a = tf.expand_dims(a,1)\n",
    "print(a.shape)\n",
    "b = tf.constant([[1,2],[10,20],[1,2]])\n",
    "b = tf.constant([[1,2]])\n",
    "b = tf.expand_dims(b,2)\n",
    "print(b.shape)\n",
    "c = a + b\n",
    "out = c.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[0.2, 0.5, 0.3]])\n",
    "b = tf.constant([[0.8, 0.1, 0.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.matmul(a,b, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_a = tf.expand_dims(a,0)\n",
    "_b = tf.expand_dims(b,-1)\n",
    "c = _a * _b\n",
    "d = c.numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices = tf.argmax(c, axis=1)  # this gives you indices from 0 to 600^2\n",
    "col_indices = indices / 3\n",
    "row_indices = indices % 3\n",
    "final_indices = tf.transpose(tf.stack(col_indices, row_indices))\n",
    "final_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.argmax(d,axis=[0,1])\n",
    "d.argmax(axis=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax_2d(tensor):\n",
    "\n",
    "  # input format: BxHxWxD\n",
    "  assert rank(tensor) == 4\n",
    "\n",
    "  # flatten the Tensor along the height and width axes\n",
    "  flat_tensor = tf.reshape(tensor, (tf.shape(tensor)[0], -1, tf.shape(tensor)[3]))\n",
    "\n",
    "  # argmax of the flat tensor\n",
    "  argmax = tf.cast(tf.argmax(flat_tensor, axis=1), tf.int32)\n",
    "\n",
    "  # convert indexes into 2D coordinates\n",
    "  argmax_x = argmax // tf.shape(tensor)[2]\n",
    "  argmax_y = argmax % tf.shape(tensor)[2]\n",
    "\n",
    "  # stack and return 2D coordinates\n",
    "  return tf.stack((argmax_x, argmax_y), axis=1)\n",
    "\n",
    "def rank(tensor):\n",
    "\n",
    "  # return the rank of a Tensor\n",
    "  return len(tensor.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax_2d(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gather(tensor, indices):\n",
    "  \"\"\"Gather in batch from a tensor of arbitrary size.\n",
    "\n",
    "  In pseudocode this module will produce the following:\n",
    "  output[i] = tf.gather(tensor[i], indices[i])\n",
    "\n",
    "  Args:\n",
    "    tensor: Tensor of arbitrary size.\n",
    "    indices: Vector of indices.\n",
    "  Returns:\n",
    "    output: A tensor of gathered values.\n",
    "  \"\"\"\n",
    "  shape = get_shape(tensor)\n",
    "  flat_first = tf.reshape(tensor, [shape[0] * shape[1]] + shape[2:])\n",
    "  indices = tf.convert_to_tensor(indices)\n",
    "  offset_shape = [shape[0]] + [1] * (indices.shape.ndims - 1)\n",
    "  offset = tf.reshape(tf.range(shape[0]) * shape[1], offset_shape)\n",
    "  output = tf.gather(flat_first, indices + offset)\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#####accuracy metric#####\n",
    "#########################\n",
    "\n",
    "tf.reset_default_graph()\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import metrics\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    start_ix = tf.expand_dims(tf.constant([10,20,30,40,50]),1)\n",
    "    end_ix =  tf.expand_dims(tf.constant([10,20,30,40,50]),1)\n",
    "    start_positions = tf.expand_dims(tf.constant([10,20,30,40,50]),1)\n",
    "    end_positions = tf.expand_dims(tf.constant([10,20,30,40,60]),1) #80% accuracy\n",
    "\n",
    "    y_pred = tf.concat([start_ix, end_ix], axis=-1) #[batch_size, 2]\n",
    "    y_true = tf.concat([start_positions, end_positions], axis=-1) #[batch_size, 2]\n",
    "    acc = tf.reduce_all(math_ops.equal(y_true, y_pred), axis=-1)\n",
    "    is_correct = math_ops.to_float(acc)\n",
    "    a,b = metrics.mean(is_correct)\n",
    "    \n",
    "    \n",
    "    running_vars = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES)\n",
    "    running_vars_initializer = tf.variables_initializer(var_list=running_vars)\n",
    "    \n",
    "    sess.run(running_vars_initializer)\n",
    "    \n",
    "    # initial op\n",
    "    a_out = sess.run(a)\n",
    "    # update op\n",
    "    b_out = sess.run(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_out, b_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from preprocessing.preprocessing import *\n",
    "from run_nq import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_data_dir = \"/data/nq/natural_questions/v1.0/sample_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_files = [_file for _file in os.listdir(bert_data_dir) if _file.endswith(\".tf_record\")]\n",
    "_file_path = [os.path.join(bert_data_dir, _file) for _file in train_files]\n",
    "[tf.gfile.MakeDirs(_dir) for _dir in [_train_path, _dev_path]]\n",
    "print(_file_path)\n",
    "train_input_fn = input_fn_builder(\n",
    "    input_file=_file_path,\n",
    "    seq_length=512,\n",
    "    is_training=True,\n",
    "    drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['batch_size'] = 32\n",
    "_iter = train_input_fn(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_iter = _iter.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = _iter.get_next()\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt['document_tokens'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = convert_examples_to_features(dt, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer\n",
    "if outputs:\n",
    "    print(outputs[0].targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{i: t for i, t in enumerate(outputs[0].tokens) if i >= outputs[0].targets[0][0] and i <= outputs[0].targets[0][1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_answer_start = dt['annotations'][0]['short_answers'][0]['start_byte']\n",
    "short_answer_end = dt['annotations'][0]['short_answers'][0]['end_byte']\n",
    "[t for t in dt['document_tokens'] if t['start_byte'] >= short_answer_start and t['end_byte'] <= short_answer_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_answer_start = dt['annotations'][0]['long_answer']['start_byte']\n",
    "long_answer_end = dt['annotations'][0]['long_answer']['end_byte']\n",
    "end = dt['annotations'][0]['long_answer']['end_token']+1\n",
    "start = dt['annotations'][0]['long_answer']['start_token']\n",
    "dt['document_tokens'][start:end]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Wordpiece\n",
    "1 - wordpiece the 'document_tokens' (remove html?)\n",
    "2 - rearrrange long and short answer byte indices accordingly.\n",
    "3 - above logic re-produces the entire text, which is too large. need to do\n",
    "    ([CLS], tokenized question, [SEP], token from content, [SEP].) -> 512 dimensions\n",
    "    \n",
    "the paper only collects short spans but falls back to long span if short span is not available. \n",
    "take examples where both short **and** long exists\n",
    "(35% of the dataset. another 37% where long answer is not available but good question.)\n",
    "\n",
    "------------------\n",
    "for each doc:\n",
    "    wordpiece the question. determine the span. this is dimensions minus the question minus special chars.\n",
    "    wordpiece the 'document_tokens' (remove html?)\n",
    "    rearrrange long and short answer byte indices accordingly.\n",
    "    \n",
    "    for each span:\n",
    "        slide the window. determine if short answer / long answer (?) is contained in the span.\n",
    "        could be multiple short answers.\n",
    "        adjust the ix accordingly. if not mark is as null / [CLS]?\n",
    "    downsample null instances\n",
    "    dump json\n",
    "    \n",
    "-------------------\n",
    "how do you train on both long and short answers? the paper emits short answers and relies on DOM to extract long answers. \n",
    "In addition, at inference time, the authors change the answers to **long only** or **no answers** based on QA eval script. \n",
    "\n",
    "long answer must be within a HTML bounding box (a paragraph or a table).\\\n",
    "\n",
    "document-QA, which is a baseline, makes a prediction but whether the prediction is long or short is determined by the inclusion of start / end of passage tokens.\n",
    "\n",
    "whether long answer exists is determined by the threshold.\n",
    "or in document-qa. predict. if the span goes over a passage, that is the long answers. and short answer does not exist?\n",
    "\n",
    "a single answer to predict on. if short answer, use. if long answer, use thsat. \n",
    "\n",
    "long answer only: 13%\n",
    "long and short: 35%\n",
    "\n",
    "\n",
    "--------------------\n",
    "how do i do it multithreaded? dask\n",
    "--------------------\n",
    "BERT: question has a positional embedding A, and answer has B.\n",
    "\n",
    "--------------------\n",
    "need to prredict if the answer is NULL. done through some post-processing heuristics on eval script score cutoff?\n",
    "\n",
    "--------------------\n",
    "what is a score.\n",
    "\n",
    "--------------------\n",
    "preprocessing should take something like a dict of labels and map them onto the new. this will apply to both problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['document_tokens'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set([t['token'] for dt in data for t in dt['document_tokens'] if t['html_token']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(dt['document_html']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt['long_answer_candidates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['question_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['annotations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = dt['document_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token for token in tokens if token['start_byte'] >= start_byte_ix and token['end_byte'] <= end_byte_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' '.join([token['token'] for token in tokens if token['start_byte'] >= start_byte_ix_long and token['end_byte'] <= end_byte_ix_long and not token['html_token']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpuenv",
   "language": "python",
   "name": "tensorflow_gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
