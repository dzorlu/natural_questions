{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "# from bert import run_classifier\n",
    "# from bert import optimization\n",
    "from bert import tokenization\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import os\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "from tensorflow.metrics import accuracy\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def span_accuracy(predictions, positions, n_way=5):\n",
    "    \"\"\"\n",
    "    Exact span match.\n",
    "    :param predictions: [batch_size, 2]\n",
    "    :param positions: [batch_size, 5, 2]\n",
    "    :return: a tuple of:\n",
    "        accuracy: A `Tensor` representing the accuracy, the value of `total` divided\n",
    "          by `count`.\n",
    "        update_op: An operation that increments the `total` and `count` variables\n",
    "          appropriately and whose value matches `accuracy`.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = tf.stack(n_way * [predictions], axis=1)\n",
    "    is_correct = tf.reduce_any(tf.equal(tf.reduce_sum(tf.cast(math_ops.equal(predictions, positions), tf.int64),axis=-1), 2),axis=-1)\n",
    "    return is_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=645, shape=(3, 5, 2), dtype=int32, numpy=\n",
       "array([[[ 1,  2],\n",
       "        [ 4,  5],\n",
       "        [ 3,  5],\n",
       "        [ 3,  5],\n",
       "        [ 1,  5]],\n",
       "\n",
       "       [[10, 11],\n",
       "        [10, 11],\n",
       "        [ 9, 12],\n",
       "        [ 3,  5],\n",
       "        [ 3,  5]],\n",
       "\n",
       "       [[30, 31],\n",
       "        [29, 39],\n",
       "        [ 9, 12],\n",
       "        [ 3,  5],\n",
       "        [ 3,  5]]], dtype=int32)>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import metrics\n",
    "n_way = 5\n",
    "predictions = tf.constant([[1,2],[10,12],[30,30]])\n",
    "positions = tf.constant([[[1,2],[4,5],[3,5],[3,5],[1,5]],\n",
    "                        [[10,11],[10,11],[9,12],[3,5],[3,5]],\n",
    "                        [[30,31],[29,39],[9,12],[3,5],[3,5]]])\n",
    "#predictions = tf.stack(n_way * [predictions], axis=1)\n",
    "#math_ops.equal(predictions, positions)\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=642, shape=(3,), dtype=bool, numpy=array([ True, False, False])>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_accuracy(predictions, positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=652, shape=(3, 2), dtype=int32, numpy=\n",
       "array([[ 1,  2],\n",
       "       [10, 11],\n",
       "       [30, 31]], dtype=int32)>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_l = tf.constant([[0.1, 0.4, 0.5],[0.1, 0.4, 0.5],[0.1, 0.9, 0.5]])\n",
    "end_l = tf.constant([[0.1, 0.4, 0.5],[0.5, 0.4, 0.1],[0.1, 0.9, 0.5]])\n",
    "\n",
    "start_l = tf.math.exp(start_l)\n",
    "end_l = tf.math.exp(end_l)\n",
    "start_l = tf.expand_dims(start_l, 1)\n",
    "end_l = tf.expand_dims(end_l, -1)\n",
    "#TODO: mask end preceeding statt.\n",
    "logits = start_l * end_l\n",
    "flat_logits = tf.reshape(logits, shape=[tf.shape(logits)[0], -1])\n",
    "logits = tf.linalg.LinearOperatorLowerTriangular(logits).to_dense()\n",
    "flat_logits = tf.reshape(logits, shape=[tf.shape(logits)[0], -1])\n",
    "_argmax = tf.cast(tf.argmax(flat_logits, axis=-1), dtype=tf.int32)\n",
    "ix = tf.cast(tf.stack([_argmax % tf.shape(logits)[1], _argmax // tf.shape(logits)[2]], axis=-1), dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "### Testing the preprocessing module   ###\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.preprocessing import convert_example\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                                  tokenization_info[\"do_lower_case\"]])\n",
    "\n",
    "    return bert.tokenization.FullTokenizer(\n",
    "        vocab_file=vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /var/folders/lc/hb9bf06j1rxbfpk731_nt7g40000gp/T/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "token = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:/Users/deniz/natural_questions/data/v1.0_sample_nq-dev-sample.jsonl:0\n",
      "INFO:tensorflow:[(0, (161, 179)), (1, (56, 180))]\n",
      "INFO:tensorflow:[(0, (239, 246)), (0, (239, 246)), (0, (239, 243)), (0, (239, 243))]\n",
      "INFO:tensorflow:[(0, (111, 118)), (0, (111, 118)), (0, (111, 115)), (0, (111, 115))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (328, 331)), (0, (328, 331)), (0, (328, 331)), (0, (328, 331)), (0, (328, 331))]\n",
      "INFO:tensorflow:[(0, (200, 203)), (0, (200, 203)), (0, (200, 203)), (0, (200, 203)), (0, (200, 203))]\n",
      "INFO:tensorflow:[(0, (72, 75)), (0, (72, 75)), (0, (72, 75)), (0, (72, 75)), (0, (72, 75))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (324, 335)), (0, (324, 335)), (0, (324, 335))]\n",
      "INFO:tensorflow:[(0, (196, 207)), (0, (196, 207)), (0, (196, 207))]\n",
      "INFO:tensorflow:[(0, (68, 79)), (0, (68, 79)), (0, (68, 79))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (66, 72)), (0, (66, 75)), (0, (66, 75))]\n",
      "INFO:tensorflow:[(0, (355, 362)), (0, (355, 362))]\n",
      "INFO:tensorflow:[(0, (227, 234)), (0, (227, 234))]\n",
      "INFO:tensorflow:[(0, (99, 106)), (0, (99, 106))]\n",
      "INFO:tensorflow:[(1, (156, 334)), (0, (183, 193)), (1, (156, 334))]\n",
      "INFO:tensorflow:[(1, (28, 206)), (0, (55, 65)), (1, (28, 206))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (262, 272)), (1, (250, 324))]\n",
      "INFO:tensorflow:[(0, (134, 144)), (1, (122, 196))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (264, 267)), (0, (264, 267)), (0, (264, 267))]\n",
      "INFO:tensorflow:[(0, (136, 139)), (0, (136, 139)), (0, (136, 139))]\n",
      "INFO:tensorflow:[(0, (255, 255)), (0, (255, 255))]\n",
      "INFO:tensorflow:[(0, (348, 352)), (0, (127, 127)), (0, (127, 127))]\n",
      "INFO:tensorflow:[(1, (147, 265)), (1, (147, 265))]\n",
      "INFO:tensorflow:[(1, (19, 137)), (1, (19, 137))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(1, (25, 314)), (0, (70, 76)), (1, (25, 314))]\n",
      "INFO:tensorflow:[(1, (140, 314)), (0, (159, 165))]\n",
      "INFO:tensorflow:[(1, (12, 186)), (0, (31, 37))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (41, 47)), (0, (41, 47)), (0, (41, 47)), (0, (41, 47)), (0, (41, 47))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (212, 212)), (0, (212, 213)), (0, (228, 228)), (0, (228, 228)), (1, (93, 242))]\n",
      "INFO:tensorflow:[(0, (84, 84)), (0, (84, 85)), (0, (100, 100)), (0, (100, 100))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(1, (19, 260)), (0, (29, 249)), (1, (19, 260)), (1, (19, 260)), (1, (19, 260))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(1, (207, 293)), (1, (207, 293)), (1, (207, 293)), (1, (207, 293)), (1, (207, 293))]\n",
      "INFO:tensorflow:[(1, (79, 165)), (1, (79, 165)), (1, (79, 165)), (1, (79, 165)), (1, (79, 165))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(1, (64, 212)), (0, (357, 366)), (0, (358, 366)), (0, (358, 366)), (0, (359, 361))]\n",
      "INFO:tensorflow:[(0, (229, 238)), (0, (230, 238)), (0, (230, 238)), (0, (231, 233))]\n",
      "INFO:tensorflow:[(0, (101, 110)), (0, (102, 110)), (0, (102, 110)), (0, (103, 105))]\n",
      "INFO:tensorflow:[(0, (345, 353)), (0, (367, 369)), (0, (345, 371))]\n",
      "INFO:tensorflow:[(0, (217, 225)), (0, (239, 241)), (0, (217, 243))]\n",
      "INFO:tensorflow:[(0, (89, 97)), (0, (111, 113)), (0, (89, 115))]\n",
      "INFO:tensorflow:[(0, (287, 288)), (0, (287, 288))]\n",
      "INFO:tensorflow:[(0, (159, 160)), (0, (159, 160))]\n",
      "INFO:tensorflow:[(0, (31, 32)), (0, (31, 32))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (175, 177)), (0, (175, 177)), (1, (133, 240)), (0, (175, 177)), (0, (175, 177))]\n",
      "INFO:tensorflow:[(0, (47, 49)), (0, (47, 49)), (0, (47, 49)), (0, (47, 49))]\n",
      "INFO:tensorflow:[(1, (38, 82)), (0, (79, 81)), (1, (38, 82)), (1, (38, 82))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (270, 273)), (0, (270, 273)), (0, (270, 273)), (0, (270, 273))]\n",
      "INFO:tensorflow:[(0, (142, 145)), (0, (142, 145)), (0, (142, 145)), (0, (142, 145))]\n",
      "INFO:tensorflow:[(0, (14, 17)), (0, (14, 17)), (0, (14, 17)), (0, (14, 17))]\n",
      "INFO:tensorflow:[(1, (319, 375)), (1, (319, 375))]\n",
      "INFO:tensorflow:[(1, (191, 247)), (1, (191, 247))]\n",
      "INFO:tensorflow:[(1, (63, 119)), (1, (63, 119))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (329, 330)), (0, (329, 330))]\n",
      "INFO:tensorflow:[(0, (201, 202)), (0, (201, 202))]\n",
      "INFO:tensorflow:[(0, (73, 74)), (0, (73, 74))]\n",
      "INFO:tensorflow:[(0, (106, 107)), (0, (106, 107))]\n",
      "INFO:tensorflow:[(0, (303, 304)), (0, (303, 304)), (0, (303, 304))]\n",
      "INFO:tensorflow:[(0, (175, 176)), (0, (175, 176)), (0, (175, 176))]\n",
      "INFO:tensorflow:[(0, (47, 48)), (0, (47, 48)), (0, (47, 48))]\n",
      "INFO:tensorflow:[(1, (219, 318)), (1, (219, 318)), (1, (219, 318)), (0, (254, 266)), (1, (219, 318))]\n",
      "INFO:tensorflow:[(1, (91, 190)), (1, (91, 190)), (1, (91, 190)), (0, (126, 138)), (1, (91, 190))]\n",
      "INFO:tensorflow:[(1, (157, 285)), (1, (157, 285)), (1, (94, 156))]\n",
      "INFO:tensorflow:[(1, (29, 157)), (1, (158, 282)), (1, (29, 157))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (286, 288)), (0, (287, 297)), (0, (286, 297)), (0, (286, 297))]\n",
      "INFO:tensorflow:[(0, (158, 160)), (0, (159, 169)), (0, (158, 169)), (0, (158, 169))]\n",
      "INFO:tensorflow:[(0, (30, 32)), (0, (31, 41)), (0, (30, 41)), (0, (30, 41))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (260, 263)), (0, (260, 263)), (0, (260, 268))]\n",
      "INFO:tensorflow:[(0, (132, 135)), (0, (132, 135)), (0, (132, 140))]\n",
      "INFO:tensorflow:[(0, (283, 283)), (0, (238, 238))]\n",
      "INFO:tensorflow:[(0, (155, 155)), (0, (110, 110))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (346, 367)), (0, (234, 243)), (0, (351, 367)), (0, (347, 367))]\n",
      "INFO:tensorflow:[(0, (218, 239)), (0, (106, 115)), (0, (218, 255)), (0, (223, 239)), (0, (219, 239))]\n",
      "INFO:tensorflow:[(0, (90, 111)), (0, (90, 127)), (0, (95, 111)), (0, (91, 111))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(1, (340, 370)), (1, (340, 370)), (1, (340, 370))]\n",
      "INFO:tensorflow:[(1, (212, 242)), (1, (212, 242)), (1, (212, 242))]\n",
      "INFO:tensorflow:[(1, (84, 114)), (1, (84, 114)), (1, (84, 114))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (296, 296)), (0, (296, 296)), (0, (296, 304))]\n",
      "INFO:tensorflow:[(0, (168, 168)), (0, (168, 168)), (0, (168, 176))]\n",
      "INFO:tensorflow:[(0, (40, 40)), (0, (40, 40)), (1, (40, 373)), (0, (40, 48))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (314, 336)), (0, (314, 336)), (0, (314, 336)), (0, (314, 336)), (0, (314, 336))]\n",
      "INFO:tensorflow:[(0, (186, 208)), (0, (186, 208)), (0, (186, 208)), (0, (186, 208)), (0, (186, 208))]\n",
      "INFO:tensorflow:[(0, (58, 80)), (0, (58, 80)), (0, (58, 80)), (0, (58, 80)), (0, (58, 80))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (30, 30)), (0, (30, 30)), (0, (30, 30)), (0, (30, 30))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (361, 361)), (0, (361, 361)), (0, (361, 361)), (0, (361, 361)), (0, (361, 361))]\n",
      "INFO:tensorflow:[(0, (233, 233)), (0, (233, 233)), (0, (233, 233)), (0, (233, 233)), (0, (233, 233))]\n",
      "INFO:tensorflow:[(0, (105, 105)), (0, (105, 105)), (0, (105, 105)), (0, (105, 105)), (0, (105, 105))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(1, (130, 347)), (1, (130, 347)), (0, (179, 203))]\n",
      "INFO:tensorflow:[(0, (46, 46)), (0, (157, 157)), (0, (157, 157)), (0, (157, 157))]\n",
      "INFO:tensorflow:[(0, (29, 29)), (0, (29, 29)), (0, (29, 29))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(1, (94, 258)), (0, (103, 110))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (93, 103)), (0, (117, 133)), (0, (93, 103))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (211, 212)), (0, (271, 272)), (0, (271, 272)), (0, (271, 272)), (0, (271, 272))]\n",
      "INFO:tensorflow:[(0, (83, 84)), (0, (143, 144)), (0, (143, 144)), (0, (143, 144)), (0, (143, 144))]\n",
      "INFO:tensorflow:[(0, (15, 16)), (0, (15, 16)), (0, (15, 16)), (0, (15, 16))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (310, 313)), (0, (310, 313)), (0, (310, 313))]\n",
      "INFO:tensorflow:[(0, (182, 185)), (0, (182, 185)), (0, (182, 185))]\n",
      "INFO:tensorflow:[(0, (54, 57)), (0, (54, 57)), (0, (54, 57))]\n",
      "INFO:tensorflow:[(0, (165, 168)), (0, (161, 168)), (0, (165, 168)), (0, (161, 163))]\n",
      "INFO:tensorflow:[(0, (37, 40)), (0, (33, 40)), (0, (37, 40)), (0, (33, 35))]\n",
      "INFO:tensorflow:[(0, (335, 339)), (0, (335, 339)), (0, (335, 339)), (0, (335, 339)), (0, (335, 339))]\n",
      "INFO:tensorflow:[(0, (207, 211)), (0, (207, 211)), (0, (207, 211)), (0, (207, 211)), (0, (207, 211))]\n",
      "INFO:tensorflow:[(0, (79, 83)), (0, (79, 83)), (0, (79, 83)), (0, (79, 83)), (0, (79, 83))]\n",
      "INFO:tensorflow:[(0, (252, 269)), (0, (262, 269)), (0, (261, 280)), (0, (262, 269))]\n",
      "INFO:tensorflow:[(0, (124, 141)), (0, (134, 141)), (0, (133, 152)), (0, (134, 141))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(1, (184, 259)), (1, (184, 259)), (1, (184, 259))]\n",
      "INFO:tensorflow:[(1, (56, 131)), (1, (56, 131)), (1, (56, 131))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (359, 361)), (0, (359, 361)), (0, (359, 361))]\n",
      "INFO:tensorflow:[(0, (231, 233)), (0, (231, 233)), (0, (231, 233))]\n",
      "INFO:tensorflow:[(0, (103, 105)), (0, (103, 105)), (0, (103, 105))]\n",
      "INFO:tensorflow:[(0, (156, 165)), (0, (156, 165)), (0, (156, 165)), (0, (156, 165))]\n",
      "INFO:tensorflow:[(0, (28, 37)), (0, (28, 37)), (0, (28, 37)), (0, (28, 37))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (368, 368)), (0, (368, 368)), (0, (368, 368))]\n",
      "INFO:tensorflow:[(0, (240, 240)), (0, (289, 289)), (0, (240, 240)), (0, (240, 240))]\n",
      "INFO:tensorflow:[(0, (112, 112)), (0, (161, 161)), (0, (112, 112)), (0, (112, 112))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (201, 219)), (1, (196, 304))]\n",
      "INFO:tensorflow:[(0, (73, 91)), (1, (68, 176))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(1, (51, 131)), (1, (132, 198))]\n",
      "INFO:tensorflow:[(0, (172, 173)), (0, (220, 221)), (0, (172, 173)), (0, (172, 173)), (0, (172, 173))]\n",
      "INFO:tensorflow:[(0, (44, 45)), (0, (92, 93)), (0, (44, 45)), (0, (44, 45)), (0, (44, 45))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(1, (64, 298)), (1, (64, 298)), (1, (64, 298)), (1, (64, 298)), (1, (64, 298))]\n",
      "INFO:tensorflow:[(0, (266, 280)), (0, (263, 280)), (0, (263, 280)), (0, (266, 280))]\n",
      "INFO:tensorflow:[(0, (138, 152)), (0, (135, 152)), (0, (135, 152)), (0, (138, 152))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (354, 359)), (1, (214, 364)), (0, (358, 359)), (0, (358, 359)), (0, (354, 359))]\n",
      "INFO:tensorflow:[(0, (226, 231)), (1, (86, 236)), (0, (230, 231)), (0, (230, 231)), (0, (226, 231))]\n",
      "INFO:tensorflow:[(0, (98, 103)), (0, (102, 103)), (0, (102, 103)), (0, (98, 103))]\n",
      "INFO:tensorflow:[(1, (23, 150)), (0, (103, 111)), (0, (103, 111)), (0, (103, 111))]\n",
      "INFO:tensorflow:[(1, (139, 283)), (1, (139, 283))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (258, 258)), (0, (173, 173)), (0, (298, 298)), (0, (298, 298))]\n",
      "INFO:tensorflow:[(0, (130, 130)), (0, (45, 45)), (0, (170, 170)), (0, (170, 170))]\n",
      "INFO:tensorflow:[(0, (42, 42)), (0, (42, 42))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (38, 39)), (0, (93, 95))]\n",
      "INFO:tensorflow:[(0, (275, 275)), (0, (275, 275)), (0, (275, 275)), (0, (275, 275))]\n",
      "INFO:tensorflow:[(0, (147, 147)), (0, (147, 147)), (0, (147, 147)), (0, (147, 147))]\n",
      "INFO:tensorflow:[(0, (19, 19)), (0, (19, 19)), (0, (19, 19)), (0, (19, 19))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(1, (235, 362)), (0, (345, 346)), (1, (235, 362))]\n",
      "INFO:tensorflow:[(1, (107, 234)), (0, (217, 218)), (1, (107, 234))]\n",
      "INFO:tensorflow:[(0, (225, 233)), (1, (212, 249)), (1, (212, 249)), (0, (225, 233)), (0, (225, 233))]\n",
      "INFO:tensorflow:[(0, (97, 105)), (1, (84, 121)), (1, (84, 121)), (0, (97, 105)), (0, (97, 105))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(1, (24, 135)), (0, (41, 55)), (1, (24, 135)), (0, (40, 55)), (1, (24, 135))]\n",
      "INFO:tensorflow:[(0, (199, 203)), (0, (195, 203)), (0, (195, 203)), (1, (183, 294)), (0, (199, 203))]\n",
      "INFO:tensorflow:[(0, (71, 75)), (0, (67, 75)), (0, (67, 75)), (1, (55, 166)), (0, (71, 75))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (282, 284)), (0, (282, 284)), (0, (282, 284)), (0, (282, 284)), (0, (282, 284))]\n",
      "INFO:tensorflow:[(0, (154, 156)), (0, (154, 156)), (0, (154, 156)), (0, (154, 156)), (0, (154, 156))]\n",
      "INFO:tensorflow:[(0, (26, 28)), (0, (26, 28)), (0, (26, 28)), (0, (26, 28)), (0, (26, 28))]\n",
      "INFO:tensorflow:[(0, (286, 290)), (0, (286, 290)), (0, (286, 290)), (0, (286, 290))]\n",
      "INFO:tensorflow:[(0, (158, 162)), (0, (158, 162)), (0, (158, 162)), (0, (158, 162))]\n",
      "INFO:tensorflow:[(0, (30, 34)), (0, (30, 34)), (0, (30, 34)), (0, (30, 34))]\n",
      "INFO:tensorflow:[(0, (283, 288)), (0, (283, 288))]\n",
      "INFO:tensorflow:[(0, (155, 160)), (0, (155, 160))]\n",
      "INFO:tensorflow:[(0, (27, 32)), (0, (27, 32))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(1, (233, 256)), (0, (252, 255))]\n",
      "INFO:tensorflow:[(1, (105, 128)), (0, (124, 127))]\n",
      "INFO:tensorflow:[(1, (115, 361)), (1, (115, 361)), (1, (115, 361)), (1, (115, 361))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(1, (281, 362)), (1, (281, 362)), (1, (281, 362)), (1, (281, 362))]\n",
      "INFO:tensorflow:[(1, (153, 234)), (1, (153, 234)), (1, (153, 234)), (1, (153, 234))]\n",
      "INFO:tensorflow:[(1, (25, 106)), (1, (25, 106)), (1, (25, 106)), (1, (25, 106))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (169, 171)), (0, (168, 171)), (0, (169, 171)), (0, (240, 242))]\n",
      "INFO:tensorflow:[(0, (41, 43)), (0, (40, 43)), (0, (41, 43)), (0, (112, 114))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (371, 378)), (0, (371, 378)), (0, (371, 378))]\n",
      "INFO:tensorflow:[(0, (243, 255)), (0, (243, 250)), (0, (307, 314)), (0, (243, 250)), (0, (243, 250))]\n",
      "INFO:tensorflow:[(0, (115, 127)), (0, (115, 122)), (0, (179, 186)), (0, (115, 122)), (0, (115, 122))]\n",
      "INFO:tensorflow:[(0, (349, 351)), (0, (349, 351)), (0, (355, 356)), (0, (350, 351)), (0, (350, 351))]\n",
      "INFO:tensorflow:[(0, (221, 223)), (0, (221, 223)), (0, (227, 228)), (0, (222, 223)), (0, (222, 223))]\n",
      "INFO:tensorflow:[(0, (93, 95)), (0, (93, 95)), (0, (99, 100)), (0, (94, 95)), (0, (94, 95))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(1, (52, 276)), (1, (52, 276)), (1, (52, 276)), (1, (52, 276))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (134, 135)), (0, (134, 135)), (0, (50, 51)), (0, (134, 135)), (0, (134, 135))]\n",
      "INFO:tensorflow:[(0, (81, 81)), (1, (69, 170)), (0, (81, 81)), (1, (69, 170)), (0, (81, 81))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (202, 203)), (0, (202, 203)), (0, (202, 203)), (1, (91, 188)), (0, (202, 203))]\n",
      "INFO:tensorflow:[(0, (74, 75)), (0, (74, 75)), (0, (74, 75)), (0, (74, 75))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (311, 370)), (0, (311, 330)), (1, (137, 371)), (1, (137, 371))]\n",
      "INFO:tensorflow:[(0, (183, 242)), (0, (183, 202))]\n",
      "INFO:tensorflow:[(0, (55, 114)), (0, (55, 74))]\n",
      "INFO:tensorflow:[(0, (295, 296)), (0, (295, 296)), (0, (295, 296)), (0, (295, 296))]\n",
      "INFO:tensorflow:[(0, (167, 168)), (0, (167, 168)), (0, (167, 168)), (0, (167, 168))]\n",
      "INFO:tensorflow:[(0, (39, 40)), (0, (39, 40)), (0, (39, 40)), (0, (39, 40))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (197, 198)), (0, (43, 44)), (0, (192, 198))]\n",
      "INFO:tensorflow:[(0, (69, 70)), (0, (64, 70))]\n",
      "INFO:tensorflow:[(0, (273, 286)), (1, (171, 287)), (1, (171, 287)), (0, (272, 286))]\n",
      "INFO:tensorflow:[(0, (145, 158)), (1, (43, 159)), (1, (43, 159)), (0, (144, 158))]\n",
      "INFO:tensorflow:[(0, (17, 30)), (0, (16, 30))]\n",
      "INFO:tensorflow:[(0, (259, 259)), (0, (259, 259)), (0, (259, 259))]\n",
      "INFO:tensorflow:[(0, (131, 131)), (0, (131, 131)), (0, (131, 131))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (254, 266)), (0, (254, 266)), (0, (254, 266)), (0, (254, 266))]\n",
      "INFO:tensorflow:[(0, (126, 138)), (0, (126, 138)), (0, (126, 138)), (0, (126, 138))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (331, 340)), (0, (331, 340))]\n",
      "INFO:tensorflow:[(0, (203, 212)), (0, (203, 212))]\n",
      "INFO:tensorflow:[(0, (75, 84)), (0, (75, 84))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (264, 270)), (0, (265, 270)), (1, (236, 271)), (1, (287, 333)), (0, (307, 313))]\n",
      "INFO:tensorflow:[(0, (136, 142)), (0, (137, 142)), (1, (108, 143)), (1, (159, 205)), (0, (179, 185))]\n",
      "INFO:tensorflow:[(1, (31, 77)), (0, (51, 57))]\n",
      "INFO:tensorflow:[(0, (352, 353)), (0, (352, 354)), (0, (352, 354)), (0, (352, 354))]\n",
      "INFO:tensorflow:[(0, (224, 225)), (0, (224, 226)), (0, (224, 226)), (0, (224, 226))]\n",
      "INFO:tensorflow:[(0, (96, 97)), (0, (96, 98)), (0, (96, 98)), (0, (96, 98))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (39, 40)), (0, (39, 40)), (0, (39, 40)), (0, (39, 40)), (0, (39, 40))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (106, 107)), (1, (100, 255)), (1, (100, 255))]\n",
      "INFO:tensorflow:[(1, (286, 354)), (1, (286, 354)), (0, (299, 303)), (1, (286, 354)), (0, (299, 303))]\n",
      "INFO:tensorflow:[(1, (158, 226)), (1, (158, 226)), (0, (171, 175)), (1, (158, 226)), (0, (171, 175))]\n",
      "INFO:tensorflow:[(1, (30, 98)), (1, (30, 98)), (0, (43, 47)), (1, (30, 98)), (0, (43, 47))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (262, 265)), (0, (267, 271))]\n",
      "INFO:tensorflow:[(0, (134, 137)), (0, (139, 143))]\n",
      "INFO:tensorflow:[(0, (195, 198)), (0, (52, 55)), (0, (52, 55)), (0, (195, 198)), (0, (195, 198))]\n",
      "INFO:tensorflow:[(0, (67, 70)), (0, (67, 70)), (0, (67, 70))]\n",
      "INFO:tensorflow:[(0, (326, 333)), (0, (326, 333)), (0, (326, 344)), (1, (155, 356)), (0, (328, 333))]\n",
      "INFO:tensorflow:[(0, (198, 205)), (0, (198, 205)), (0, (198, 216)), (1, (27, 228)), (0, (200, 205))]\n",
      "INFO:tensorflow:[(0, (70, 77)), (0, (70, 77)), (0, (70, 88)), (0, (72, 77))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(0, (80, 98)), (0, (81, 106)), (0, (81, 84)), (0, (86, 106))]\n",
      "INFO:tensorflow:[(0, (118, 126)), (0, (119, 122)), (0, (118, 126)), (0, (118, 122)), (0, (119, 122))]\n",
      "INFO:tensorflow:[(1, (176, 279)), (1, (176, 197))]\n",
      "INFO:tensorflow:[(1, (48, 151)), (1, (48, 69))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n",
      "INFO:tensorflow:[(2, (0, 0))]\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "_train_file = '/Users/deniz/natural_questions/data/v1.0_sample_nq-train-sample.jsonl'\n",
    "_dev_file = '/Users/deniz/natural_questions/data/v1.0_sample_nq-dev-sample.jsonl'\n",
    "with jsonlines.open(_dev_file) as reader:\n",
    "    features, examples = [], []\n",
    "    for i, example in enumerate(reader):\n",
    "        if i % 1e3 == 0: tf.logging.info(\"{}:{}\".format(_dev_file, i))\n",
    "        examples.append(example)\n",
    "        dt = convert_example(example=example,\n",
    "                             tokenizer=token,\n",
    "                             mode='eval',\n",
    "                             max_seq_length=384,\n",
    "                             doc_stride=128,\n",
    "                             max_query_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "_train_file = '/Users/deniz/natural_questions/data/v1.0_sample_nq-train-sample.jsonl'\n",
    "_dev_file = '/Users/deniz/natural_questions/data/v1.0_sample_nq-dev-sample.jsonl'\n",
    "with jsonlines.open(_train_file) as reader:\n",
    "    features, train_examples = [], []\n",
    "    for i, example in enumerate(reader):\n",
    "        train_examples.append(example['annotations'])\n",
    "\n",
    "            \n",
    "_train_file = '/Users/deniz/natural_questions/data/v1.0_sample_nq-train-sample.jsonl'\n",
    "_dev_file = '/Users/deniz/natural_questions/data/v1.0_sample_nq-dev-sample.jsonl'\n",
    "with jsonlines.open(_dev_file) as reader:\n",
    "    features, dev_examples = [], []\n",
    "    for i, example in enumerate(reader):\n",
    "        dev_examples.append(example['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({1: 200}), Counter({5: 200}))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter([len(t) for t in train_examples]), Counter([len(t) for t in dev_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'annotation_id': 7719528322202775345,\n",
       "  'long_answer': {'candidate_index': 58,\n",
       "   'end_byte': 84070,\n",
       "   'end_token': 965,\n",
       "   'start_byte': 81195,\n",
       "   'start_token': 809},\n",
       "  'short_answers': [{'end_byte': 82556,\n",
       "    'end_token': 838,\n",
       "    'start_byte': 81281,\n",
       "    'start_token': 814}],\n",
       "  'yes_no_answer': 'NONE'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 130, 1: 69, 3: 1})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple short answers per annotation\n",
    "Counter([len(t[0]['short_answers']) for t in train_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'annotation_id': 10957934160137332476,\n",
       "  'long_answer': {'candidate_index': -1,\n",
       "   'end_byte': -1,\n",
       "   'end_token': -1,\n",
       "   'start_byte': -1,\n",
       "   'start_token': -1},\n",
       "  'short_answers': [],\n",
       "  'yes_no_answer': 'NONE'},\n",
       " {'annotation_id': 2807704282985816749,\n",
       "  'long_answer': {'candidate_index': -1,\n",
       "   'end_byte': -1,\n",
       "   'end_token': -1,\n",
       "   'start_byte': -1,\n",
       "   'start_token': -1},\n",
       "  'short_answers': [],\n",
       "  'yes_no_answer': 'NONE'},\n",
       " {'annotation_id': 5129692602407601925,\n",
       "  'long_answer': {'candidate_index': 0,\n",
       "   'end_byte': 57005,\n",
       "   'end_token': 122,\n",
       "   'start_byte': 56251,\n",
       "   'start_token': 43},\n",
       "  'short_answers': [],\n",
       "  'yes_no_answer': 'NONE'},\n",
       " {'annotation_id': 4965838886380681126,\n",
       "  'long_answer': {'candidate_index': 90,\n",
       "   'end_byte': 104818,\n",
       "   'end_token': 4593,\n",
       "   'start_byte': 104066,\n",
       "   'start_token': 4485},\n",
       "  'short_answers': [{'end_byte': 104576,\n",
       "    'end_token': 4570,\n",
       "    'start_byte': 104572,\n",
       "    'start_token': 4569},\n",
       "   {'end_byte': 104639,\n",
       "    'end_token': 4572,\n",
       "    'start_byte': 104633,\n",
       "    'start_token': 4571}],\n",
       "  'yes_no_answer': 'NONE'},\n",
       " {'annotation_id': 9518639313383123593,\n",
       "  'long_answer': {'candidate_index': -1,\n",
       "   'end_byte': -1,\n",
       "   'end_token': -1,\n",
       "   'start_byte': -1,\n",
       "   'start_token': -1},\n",
       "  'short_answers': [],\n",
       "  'yes_no_answer': 'NONE'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_examples[-2]['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2975172535563055798"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = features[10]\n",
    "feature.example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(i):\n",
    "    feature = features[i]\n",
    "    example_id = feature.example_id\n",
    "    example = [x for x in examples if x['example_id'] == example_id][0]\n",
    "    return feature, example  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotations(example):\n",
    "    \"\"\"\n",
    "    if short, else long\n",
    "    \"\"\"\n",
    "    annotation = example['annotations'][0]\n",
    "    end_byte_ix, start_byte_ix = None, None\n",
    "    start_token, end_token = None, None\n",
    "    if annotation['short_answers']:\n",
    "        end_byte_ix = annotation['short_answers'][0]['end_byte']\n",
    "        start_token = annotation['short_answers'][0]['start_token']\n",
    "        end_token = annotation['short_answers'][0]['end_token']\n",
    "        start_byte_ix = annotation['short_answers'][0]['start_byte']\n",
    "    else:\n",
    "        end_byte_ix = annotation['long_answer']['end_byte']\n",
    "        start_byte_ix = annotation['long_answer']['start_byte']\n",
    "        start_token = annotation['long_answer']['start_token']\n",
    "        end_token = annotation['long_answer']['end_token']\n",
    "    return {'end_byte_ix': end_byte_ix, \n",
    "            'start_byte_ix': start_byte_ix,\n",
    "            'start_token': start_token,\n",
    "            'end_token': end_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate(i):\n",
    "    # get the feature and example the feature is derived from.\n",
    "    feature, example = test(i)\n",
    "    # get the ground truth annotations.\n",
    "    gt = get_annotations(example)\n",
    "    # get start byte and end bytes for targets.\n",
    "    if feature.targets[0] == 0:\n",
    "        return (i, True)\n",
    "    start_bytes = feature.start_bytes[feature.targets[0]]\n",
    "    end_bytes = feature.end_bytes[feature.targets[1]]\n",
    "    if start_bytes == gt['start_byte_ix'] and end_bytes == gt['end_byte_ix']:\n",
    "        return (i,True)\n",
    "    else:\n",
    "        return (i, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'end_byte_ix': 96731, 'start_byte_ix': 96715, 'start_token': 3521, 'end_token': 3525}\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, True)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature, example = test(1)\n",
    "gt = get_annotations(example)\n",
    "print(gt)\n",
    "start_bytes = feature.start_bytes[feature.targets[0]]\n",
    "end_bytes = feature.end_bytes[feature.targets[1]]\n",
    "feature.targets, start_bytes, end_bytes\n",
    "_validate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = []\n",
    "for i in range(len(features)):\n",
    "    _assertion =  _validate(i)\n",
    "    if not _assertion[1]:\n",
    "        ix.append(i)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth\n",
      "{'end_byte_ix': 55798, 'start_byte_ix': 55137, 'start_token': 893, 'end_token': 1001}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55140, 55794)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assertion fails\n",
    "feature, example = test(ix[2])\n",
    "gt = get_annotations(example)\n",
    "print('ground truth')\n",
    "print(gt)\n",
    "start_bytes = feature.start_bytes[feature.targets[0]]\n",
    "end_bytes = feature.end_bytes[feature.targets[1]]\n",
    "start_bytes, end_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'annotation_id': 13306123758205215060,\n",
       "  'long_answer': {'candidate_index': 32,\n",
       "   'end_byte': 55798,\n",
       "   'end_token': 1001,\n",
       "   'start_byte': 55137,\n",
       "   'start_token': 893},\n",
       "  'short_answers': [],\n",
       "  'yes_no_answer': 'NONE'}]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'end_byte': 55140, 'html_token': True, 'start_byte': 55137, 'token': '<P>'},\n",
       " {'end_byte': 55143, 'html_token': False, 'start_byte': 55140, 'token': 'The'},\n",
       " {'end_byte': 55153,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55144,\n",
       "  'token': 'marooning'},\n",
       " {'end_byte': 55156, 'html_token': False, 'start_byte': 55154, 'token': 'of'},\n",
       " {'end_byte': 55167,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55160,\n",
       "  'token': 'Voyager'},\n",
       " {'end_byte': 55174, 'html_token': False, 'start_byte': 55172, 'token': 'in'},\n",
       " {'end_byte': 55178, 'html_token': False, 'start_byte': 55175, 'token': 'the'},\n",
       " {'end_byte': 55184,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55179,\n",
       "  'token': 'Delta'},\n",
       " {'end_byte': 55193,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55185,\n",
       "  'token': 'Quadrant'},\n",
       " {'end_byte': 55202,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55194,\n",
       "  'token': 'provided'},\n",
       " {'end_byte': 55208,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55203,\n",
       "  'token': 'Paris'},\n",
       " {'end_byte': 55213,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55209,\n",
       "  'token': 'with'},\n",
       " {'end_byte': 55215, 'html_token': False, 'start_byte': 55214, 'token': 'a'},\n",
       " {'end_byte': 55219, 'html_token': False, 'start_byte': 55216, 'token': 'new'},\n",
       " {'end_byte': 55229,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55220,\n",
       "  'token': 'beginning'},\n",
       " {'end_byte': 55230, 'html_token': False, 'start_byte': 55229, 'token': '.'},\n",
       " {'end_byte': 55238,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55231,\n",
       "  'token': 'Janeway'},\n",
       " {'end_byte': 55243,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55239,\n",
       "  'token': 'gave'},\n",
       " {'end_byte': 55249,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55244,\n",
       "  'token': 'Paris'},\n",
       " {'end_byte': 55251, 'html_token': False, 'start_byte': 55250, 'token': 'a'},\n",
       " {'end_byte': 55257,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55252,\n",
       "  'token': 'field'},\n",
       " {'end_byte': 55268,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55258,\n",
       "  'token': 'commission'},\n",
       " {'end_byte': 55271, 'html_token': False, 'start_byte': 55269, 'token': 'as'},\n",
       " {'end_byte': 55273, 'html_token': False, 'start_byte': 55272, 'token': 'a'},\n",
       " {'end_byte': 55283,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55274,\n",
       "  'token': 'Starfleet'},\n",
       " {'end_byte': 55294,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55284,\n",
       "  'token': 'lieutenant'},\n",
       " {'end_byte': 55298, 'html_token': False, 'start_byte': 55295, 'token': 'and'},\n",
       " {'end_byte': 55303,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55299,\n",
       "  'token': 'made'},\n",
       " {'end_byte': 55307, 'html_token': False, 'start_byte': 55304, 'token': 'him'},\n",
       " {'end_byte': 55313,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55308,\n",
       "  'token': 'chief'},\n",
       " {'end_byte': 55322,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55314,\n",
       "  'token': 'helmsman'},\n",
       " {'end_byte': 55325, 'html_token': False, 'start_byte': 55323, 'token': 'of'},\n",
       " {'end_byte': 55336,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55329,\n",
       "  'token': 'Voyager'},\n",
       " {'end_byte': 55341, 'html_token': False, 'start_byte': 55340, 'token': '.'},\n",
       " {'end_byte': 55344, 'html_token': False, 'start_byte': 55342, 'token': 'He'},\n",
       " {'end_byte': 55348, 'html_token': False, 'start_byte': 55345, 'token': 'had'},\n",
       " {'end_byte': 55350, 'html_token': False, 'start_byte': 55349, 'token': 'a'},\n",
       " {'end_byte': 55356,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55351,\n",
       "  'token': 'rough'},\n",
       " {'end_byte': 55362,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55357,\n",
       "  'token': 'start'},\n",
       " {'end_byte': 55363, 'html_token': False, 'start_byte': 55362, 'token': ','},\n",
       " {'end_byte': 55371,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55364,\n",
       "  'token': 'however'},\n",
       " {'end_byte': 55372, 'html_token': False, 'start_byte': 55371, 'token': ','},\n",
       " {'end_byte': 55375, 'html_token': False, 'start_byte': 55373, 'token': 'as'},\n",
       " {'end_byte': 55385,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55376,\n",
       "  'token': 'Starfleet'},\n",
       " {'end_byte': 55389, 'html_token': False, 'start_byte': 55386, 'token': 'and'},\n",
       " {'end_byte': 55396,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55390,\n",
       "  'token': 'Maquis'},\n",
       " {'end_byte': 55402,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55397,\n",
       "  'token': 'alike'},\n",
       " {'end_byte': 55409,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55403,\n",
       "  'token': 'viewed'},\n",
       " {'end_byte': 55415,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55410,\n",
       "  'token': 'Paris'},\n",
       " {'end_byte': 55420,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55416,\n",
       "  'token': 'with'},\n",
       " {'end_byte': 55430,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55421,\n",
       "  'token': 'suspicion'},\n",
       " {'end_byte': 55431, 'html_token': False, 'start_byte': 55430, 'token': '.'},\n",
       " {'end_byte': 55437,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55432,\n",
       "  'token': 'Paris'},\n",
       " {'end_byte': 55444,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55438,\n",
       "  'token': 'worked'},\n",
       " {'end_byte': 55449,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55445,\n",
       "  'token': 'hard'},\n",
       " {'end_byte': 55452, 'html_token': False, 'start_byte': 55450, 'token': 'to'},\n",
       " {'end_byte': 55457,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55453,\n",
       "  'token': 'earn'},\n",
       " {'end_byte': 55461, 'html_token': False, 'start_byte': 55458, 'token': 'his'},\n",
       " {'end_byte': 55471,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55462,\n",
       "  'token': 'crewmates'},\n",
       " {'end_byte': 55476, 'html_token': False, 'start_byte': 55471, 'token': \"'\"},\n",
       " {'end_byte': 55484,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55477,\n",
       "  'token': 'respect'},\n",
       " {'end_byte': 55485, 'html_token': False, 'start_byte': 55484, 'token': '.'},\n",
       " {'end_byte': 55492,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55486,\n",
       "  'token': 'During'},\n",
       " {'end_byte': 55497,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55493,\n",
       "  'token': 'this'},\n",
       " {'end_byte': 55502,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55498,\n",
       "  'token': 'time'},\n",
       " {'end_byte': 55503, 'html_token': False, 'start_byte': 55502, 'token': ','},\n",
       " {'end_byte': 55506, 'html_token': False, 'start_byte': 55504, 'token': 'he'},\n",
       " {'end_byte': 55513,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55507,\n",
       "  'token': 'became'},\n",
       " {'end_byte': 55518,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55514,\n",
       "  'token': 'best'},\n",
       " {'end_byte': 55526,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55519,\n",
       "  'token': 'friends'},\n",
       " {'end_byte': 55531,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55527,\n",
       "  'token': 'with'},\n",
       " {'end_byte': 55538,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55532,\n",
       "  'token': 'Ensign'},\n",
       " {'end_byte': 55612,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55607,\n",
       "  'token': 'Harry'},\n",
       " {'end_byte': 55616, 'html_token': False, 'start_byte': 55613, 'token': 'Kim'},\n",
       " {'end_byte': 55621, 'html_token': False, 'start_byte': 55620, 'token': ','},\n",
       " {'end_byte': 55623, 'html_token': False, 'start_byte': 55622, 'token': 'a'},\n",
       " {'end_byte': 55629,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55624,\n",
       "  'token': 'young'},\n",
       " {'end_byte': 55637,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55630,\n",
       "  'token': 'officer'},\n",
       " {'end_byte': 55640, 'html_token': False, 'start_byte': 55638, 'token': 'on'},\n",
       " {'end_byte': 55644, 'html_token': False, 'start_byte': 55641, 'token': 'his'},\n",
       " {'end_byte': 55650,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55645,\n",
       "  'token': 'first'},\n",
       " {'end_byte': 55658,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55651,\n",
       "  'token': 'mission'},\n",
       " {'end_byte': 55662, 'html_token': False, 'start_byte': 55659, 'token': 'who'},\n",
       " {'end_byte': 55669,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55663,\n",
       "  'token': 'defied'},\n",
       " {'end_byte': 55673, 'html_token': False, 'start_byte': 55670, 'token': 'his'},\n",
       " {'end_byte': 55683,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55674,\n",
       "  'token': 'crewmates'},\n",
       " {'end_byte': 55686, 'html_token': False, 'start_byte': 55684, 'token': 'to'},\n",
       " {'end_byte': 55695,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55687,\n",
       "  'token': 'befriend'},\n",
       " {'end_byte': 55701,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55696,\n",
       "  'token': 'Paris'},\n",
       " {'end_byte': 55702, 'html_token': False, 'start_byte': 55701, 'token': '.'},\n",
       " {'end_byte': 55713,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55703,\n",
       "  'token': 'Eventually'},\n",
       " {'end_byte': 55714, 'html_token': False, 'start_byte': 55713, 'token': ','},\n",
       " {'end_byte': 55720,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55715,\n",
       "  'token': 'Paris'},\n",
       " {'end_byte': 55724, 'html_token': False, 'start_byte': 55721, 'token': 'was'},\n",
       " {'end_byte': 55733,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55725,\n",
       "  'token': 'accepted'},\n",
       " {'end_byte': 55736, 'html_token': False, 'start_byte': 55734, 'token': 'by'},\n",
       " {'end_byte': 55740, 'html_token': False, 'start_byte': 55737, 'token': 'the'},\n",
       " {'end_byte': 55745,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55741,\n",
       "  'token': 'crew'},\n",
       " {'end_byte': 55749, 'html_token': False, 'start_byte': 55746, 'token': 'and'},\n",
       " {'end_byte': 55756,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55750,\n",
       "  'token': 'became'},\n",
       " {'end_byte': 55760, 'html_token': False, 'start_byte': 55757, 'token': 'one'},\n",
       " {'end_byte': 55763, 'html_token': False, 'start_byte': 55761, 'token': 'of'},\n",
       " {'end_byte': 55771,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55764,\n",
       "  'token': 'Janeway'},\n",
       " {'end_byte': 55777, 'html_token': False, 'start_byte': 55771, 'token': \"'s\"},\n",
       " {'end_byte': 55784,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55778,\n",
       "  'token': 'valued'},\n",
       " {'end_byte': 55793,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55785,\n",
       "  'token': 'officers'},\n",
       " {'end_byte': 55794, 'html_token': False, 'start_byte': 55793, 'token': '.'},\n",
       " {'end_byte': 55798, 'html_token': True, 'start_byte': 55794, 'token': '</P>'}]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = get_annotations(example)\n",
    "[t for t in example['document_tokens'] if t['start_byte'] >= ann['start_byte_ix'] and t['end_byte'] <= ann['end_byte_ix']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_nq import input_fn_builder\n",
    "seq_length=384\n",
    "\n",
    "name_to_features = {\n",
    "  \"input_ids\": tf.FixedLenFeature([], tf.int64),\n",
    "  \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "  \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "  \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "  \"start_bytes\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "  \"end_bytes\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "}\n",
    "name_to_features[\"start_positions\"] = tf.FixedLenFeature([], tf.int64)\n",
    "name_to_features[\"end_positions\"] = tf.FixedLenFeature([], tf.int64)\n",
    "\n",
    "def _decode_record(record):\n",
    "  \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "  example = tf.parse_single_example(record, name_to_features)\n",
    "  return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import run_nq\n",
    "bert_data_dir = '/data/nq/natural_questions/v1.0/'\n",
    "_train_path = '/Users/deniz/natural_questions/data/'\n",
    "_dev_path = os.path.join(bert_data_dir, 'dev')\n",
    "_train_path = os.path.join(bert_data_dir, 'train')\n",
    "train_files = [os.path.join(_train_path, _file) for _file in os.listdir(_train_path) if _file.endswith(\".tf_record\")]\n",
    "\n",
    "train_input_fn = input_fn_builder(\n",
    "  input_files=train_files,\n",
    "  seq_length=384,\n",
    "  is_training=True,\n",
    "  mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=173, shape=(32, 384), dtype=int64, numpy=\n",
       "array([[  101,  2043,  2106, ...,  3237,  1022,   102],\n",
       "       [  101,  2040,  2209, ..., 11411,  9358,   102],\n",
       "       [  101,  2073,  2515, ...,  1998,  6887,   102],\n",
       "       ...,\n",
       "       [  101,  2040,  6369, ...,  1005,  1005,   102],\n",
       "       [  101,  2040,  2001, ...,  8884,  2000,   102],\n",
       "       [  101,  2029,  1997, ...,  2086,  1010,   102]])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "dt = tf.data.TFRecordDataset(train_files)\n",
    "dt = dt.map(_decode_record, num_parallel_calls=10)\n",
    "dt = dt.shuffle(buffer_size=100)\n",
    "dt = dt.batch(32)\n",
    "it = dt.make_one_shot_iterator()\n",
    "a = it.get_next()\n",
    "a['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=116, shape=(32,), dtype=int64, numpy=\n",
       "array([283,   0, 148,   0,   0, 290, 229,   0,  34, 162,   0,   0,  28,\n",
       "       101, 150,   0, 155,   0,   0,  27,   0,  48, 100, 304, 209,   0,\n",
       "       212,   0,   0, 131,  87,   0])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['start_positions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=111, shape=(32,), dtype=int64, numpy=\n",
       "array([341,   0, 151,   0,   0, 299, 243,   0,  43, 171,   0,   0,  47,\n",
       "       115, 152,   0, 155,   0,   0,  85,   0,  98, 101, 354, 321,   0,\n",
       "       212,   0,   0, 244,  93,   0])>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['end_positions']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/home/deniz/repos/natural_questions/model/training', '_tf_random_seed': None, '_save_summary_steps': 50, '_save_checkpoints_steps': 10, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 10, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f26bc44e278>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "######### INFERENCE    ######\n",
    "#############################\n",
    "\n",
    "from run_nq import input_fn_builder\n",
    "bert_data_dir = '/data/nq/natural_questions/v1.0/'\n",
    "from bert import modeling\n",
    "from run_nq import model_fn_builder\n",
    "# prediction\n",
    "bert_config = modeling.BertConfig.from_json_file('model/uncased_L-12_H-768_A-12/bert_config.json')\n",
    "\n",
    "_dev_path = os.path.join(bert_data_dir, 'dev')\n",
    "_train_path = os.path.join(bert_data_dir, 'train')\n",
    "_predict_path = os.path.join(bert_data_dir, 'predict')\n",
    "\n",
    "config = tf.estimator.RunConfig(\n",
    "  save_checkpoints_steps=10, # this also sets when eval starts\n",
    "  save_summary_steps=50,\n",
    "  keep_checkpoint_max=10, #train_and_eval does not save the best models, but the most recent ones.\n",
    "  model_dir='/home/deniz/repos/natural_questions/model/training'\n",
    ")\n",
    "\n",
    "\n",
    "model_fn = model_fn_builder(\n",
    "  bert_config=bert_config,\n",
    "  init_checkpoint='/home/deniz/repos/natural_questions/model/training',\n",
    "  learning_rate=1e-5,\n",
    "  num_train_steps=10,\n",
    "  num_warmup_steps=0,\n",
    "  use_tpu=False,\n",
    "  use_one_hot_embeddings=False)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=config,\n",
    "  params={'batch_size':8})\n",
    "\n",
    "\n",
    "predict_files = [os.path.join(_train_path, _file) for _file in os.listdir(_train_path) if\n",
    "                 _file.endswith(\".tf_record\")]\n",
    "predict_input_fn = input_fn_builder(\n",
    "    input_files=predict_files,\n",
    "    seq_length=384,\n",
    "    is_training=False,\n",
    "    mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Estimator.predict at 0x7f26bc43ef10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_result = estimator.predict(predict_input_fn)\n",
    "batch_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_batch_result = next(batch_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278, 342)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(_batch_result['y_pred_start']), np.argmax(_batch_result['y_pred_end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(295, 306)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GROUND TRUTH\n",
    "_batch_result['start_positions'], _batch_result['end_positions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.176463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295</td>\n",
       "      <td>2.008699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>298</td>\n",
       "      <td>3.161665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>299</td>\n",
       "      <td>4.229423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>302</td>\n",
       "      <td>0.544027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0         1\n",
       "0    0  0.176463\n",
       "1  295  2.008699\n",
       "2  298  3.161665\n",
       "3  299  4.229423\n",
       "4  302  0.544027"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits = pd.DataFrame([(i, score) for i, score in enumerate(_batch_result['start_logits']) if score > 0 ])\n",
    "start_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.770081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>306</td>\n",
       "      <td>6.001465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>325</td>\n",
       "      <td>0.931091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>335</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>356</td>\n",
       "      <td>0.244597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>357</td>\n",
       "      <td>2.219360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0         1\n",
       "0    0  0.770081\n",
       "1  306  6.001465\n",
       "2  325  0.931091\n",
       "3  335  0.933333\n",
       "4  356  0.244597\n",
       "5  357  2.219360"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_logits = pd.DataFrame([(i, score) for i, score in enumerate(_batch_result['end_logits']) if score > 0 ])\n",
    "end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.061153622438558e-09"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#####accuracy metric#####\n",
    "#########################\n",
    "\n",
    "tf.reset_default_graph()\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import metrics\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    start_ix = tf.expand_dims(tf.constant([10,20,30,40,50]),1)\n",
    "    end_ix =  tf.expand_dims(tf.constant([10,20,30,40,50]),1)\n",
    "    start_positions = tf.expand_dims(tf.constant([10,20,30,40,50]),1)\n",
    "    end_positions = tf.expand_dims(tf.constant([10,20,30,40,60]),1) #80% accuracy\n",
    "\n",
    "    y_pred = tf.concat([start_ix, end_ix], axis=-1) #[batch_size, 2]\n",
    "    y_true = tf.concat([start_positions, end_positions], axis=-1) #[batch_size, 2]\n",
    "    acc = tf.reduce_all(math_ops.equal(y_true, y_pred), axis=-1)\n",
    "    is_correct = math_ops.to_float(acc)\n",
    "    a,b = metrics.mean(is_correct)\n",
    "    \n",
    "    \n",
    "    running_vars = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES)\n",
    "    running_vars_initializer = tf.variables_initializer(var_list=running_vars)\n",
    "    \n",
    "    sess.run(running_vars_initializer)\n",
    "    \n",
    "    # initial op\n",
    "    a_out = sess.run(a)\n",
    "    # update op\n",
    "    b_out = sess.run(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_out, b_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from preprocessing.preprocessing import *\n",
    "from run_nq import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_data_dir = \"/data/nq/natural_questions/v1.0/sample_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_files = [_file for _file in os.listdir(bert_data_dir) if _file.endswith(\".tf_record\")]\n",
    "_file_path = [os.path.join(bert_data_dir, _file) for _file in train_files]\n",
    "[tf.gfile.MakeDirs(_dir) for _dir in [_train_path, _dev_path]]\n",
    "print(_file_path)\n",
    "train_input_fn = input_fn_builder(\n",
    "    input_file=_file_path,\n",
    "    seq_length=512,\n",
    "    is_training=True,\n",
    "    drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['batch_size'] = 32\n",
    "_iter = train_input_fn(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_iter = _iter.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = _iter.get_next()\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt['document_tokens'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = convert_examples_to_features(dt, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer\n",
    "if outputs:\n",
    "    print(outputs[0].targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{i: t for i, t in enumerate(outputs[0].tokens) if i >= outputs[0].targets[0][0] and i <= outputs[0].targets[0][1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_answer_start = dt['annotations'][0]['short_answers'][0]['start_byte']\n",
    "short_answer_end = dt['annotations'][0]['short_answers'][0]['end_byte']\n",
    "[t for t in dt['document_tokens'] if t['start_byte'] >= short_answer_start and t['end_byte'] <= short_answer_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_answer_start = dt['annotations'][0]['long_answer']['start_byte']\n",
    "long_answer_end = dt['annotations'][0]['long_answer']['end_byte']\n",
    "end = dt['annotations'][0]['long_answer']['end_token']+1\n",
    "start = dt['annotations'][0]['long_answer']['start_token']\n",
    "dt['document_tokens'][start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['document_tokens'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set([t['token'] for dt in data for t in dt['document_tokens'] if t['html_token']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(dt['document_html']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt['long_answer_candidates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['question_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['annotations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = dt['document_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token for token in tokens if token['start_byte'] >= start_byte_ix and token['end_byte'] <= end_byte_ix]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3tf",
   "language": "python",
   "name": "py3tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
