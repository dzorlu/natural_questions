{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "# from bert import run_classifier\n",
    "# from bert import optimization\n",
    "from bert import tokenization\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "from tensorflow.metrics import accuracy\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [7, 8, 9]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1,2,3],[4,5,6],[7,8,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=781, shape=(3,), dtype=float32, numpy=array([1., 5., 8.], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing prdiction\n",
    "y_pred = tf.cast(tf.constant([[0,1],[1,2],[1,3]]),dtype=tf.int32) # [batch_size, 2]\n",
    "a = tf.constant(np.array([[1,2],[4,5],[7,8]]),dtype=tf.float32)\n",
    "pred_start_byte, pred_end_byte = y_pred[:,0], y_pred[:,1]\n",
    "pred_start_byte = tf.cast(tf.one_hot(pred_start_byte, depth=2), dtype=tf.float32)\n",
    "tf.reduce_sum(a * pred_start_byte, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing the preprocessing module\n",
    "\n",
    "from preprocessing.preprocessing import convert_example\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                                  tokenization_info[\"do_lower_case\"]])\n",
    "\n",
    "    return bert.tokenization.FullTokenizer(\n",
    "        vocab_file=vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /var/folders/lc/hb9bf06j1rxbfpk731_nt7g40000gp/T/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "token = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:/Users/deniz/natural_questions/data/v1.0_sample_nq-train-sample.jsonl:0\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(276, 279)\n",
      "INFO:tensorflow:(148, 151)\n",
      "INFO:tensorflow:(20, 23)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(253, 302)\n",
      "INFO:tensorflow:(125, 174)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(40, 316)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(327, 332)\n",
      "INFO:tensorflow:(199, 204)\n",
      "INFO:tensorflow:(71, 76)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(314, 315)\n",
      "INFO:tensorflow:(186, 187)\n",
      "INFO:tensorflow:(58, 59)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(370, 370)\n",
      "INFO:tensorflow:(242, 242)\n",
      "INFO:tensorflow:(114, 114)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(202, 322)\n",
      "INFO:tensorflow:(74, 194)\n",
      "INFO:tensorflow:(249, 257)\n",
      "INFO:tensorflow:(121, 129)\n",
      "INFO:tensorflow:(205, 316)\n",
      "INFO:tensorflow:(77, 188)\n",
      "INFO:tensorflow:(140, 285)\n",
      "INFO:tensorflow:(12, 157)\n",
      "INFO:tensorflow:(253, 299)\n",
      "INFO:tensorflow:(125, 171)\n",
      "INFO:tensorflow:(244, 268)\n",
      "INFO:tensorflow:(116, 140)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(87, 95)\n",
      "INFO:tensorflow:(150, 267)\n",
      "INFO:tensorflow:(22, 139)\n",
      "INFO:tensorflow:(337, 337)\n",
      "INFO:tensorflow:(209, 209)\n",
      "INFO:tensorflow:(81, 81)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(170, 189)\n",
      "INFO:tensorflow:(42, 61)\n",
      "INFO:tensorflow:(179, 183)\n",
      "INFO:tensorflow:(51, 55)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(292, 295)\n",
      "INFO:tensorflow:(164, 167)\n",
      "INFO:tensorflow:(36, 39)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(56, 377)\n",
      "INFO:tensorflow:(258, 258)\n",
      "INFO:tensorflow:(130, 130)\n",
      "INFO:tensorflow:(135, 139)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(37, 66)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(36, 37)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(125, 225)\n",
      "INFO:tensorflow:(149, 153)\n",
      "INFO:tensorflow:(21, 25)\n",
      "INFO:tensorflow:(303, 309)\n",
      "INFO:tensorflow:(175, 181)\n",
      "INFO:tensorflow:(47, 53)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(293, 293)\n",
      "INFO:tensorflow:(165, 165)\n",
      "INFO:tensorflow:(37, 37)\n",
      "INFO:tensorflow:(251, 257)\n",
      "INFO:tensorflow:(123, 129)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(241, 245)\n",
      "INFO:tensorflow:(113, 117)\n",
      "INFO:tensorflow:(290, 292)\n",
      "INFO:tensorflow:(162, 164)\n",
      "INFO:tensorflow:(34, 36)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(230, 232)\n",
      "INFO:tensorflow:(102, 104)\n",
      "INFO:tensorflow:(269, 272)\n",
      "INFO:tensorflow:(141, 144)\n",
      "INFO:tensorflow:(13, 16)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(296, 361)\n",
      "INFO:tensorflow:(168, 233)\n",
      "INFO:tensorflow:(40, 105)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(95, 98)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(227, 231)\n",
      "INFO:tensorflow:(99, 103)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(119, 206)\n",
      "INFO:tensorflow:(111, 113)\n",
      "INFO:tensorflow:(170, 171)\n",
      "INFO:tensorflow:(42, 43)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(108, 111)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(310, 330)\n",
      "INFO:tensorflow:(182, 202)\n",
      "INFO:tensorflow:(54, 74)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(354, 354)\n",
      "INFO:tensorflow:(226, 226)\n",
      "INFO:tensorflow:(98, 98)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(216, 218)\n",
      "INFO:tensorflow:(88, 90)\n",
      "INFO:tensorflow:(313, 315)\n",
      "INFO:tensorflow:(185, 187)\n",
      "INFO:tensorflow:(57, 59)\n",
      "INFO:tensorflow:(191, 235)\n",
      "INFO:tensorflow:(63, 107)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(67, 68)\n",
      "INFO:tensorflow:(263, 264)\n",
      "INFO:tensorflow:(135, 136)\n",
      "INFO:tensorflow:(44, 53)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(333, 333)\n",
      "INFO:tensorflow:(205, 205)\n",
      "INFO:tensorflow:(77, 77)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(99, 101)\n",
      "INFO:tensorflow:(218, 356)\n",
      "INFO:tensorflow:(90, 228)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(375, 378)\n",
      "INFO:tensorflow:(247, 250)\n",
      "INFO:tensorflow:(119, 122)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(184, 193)\n",
      "INFO:tensorflow:(56, 65)\n",
      "INFO:tensorflow:(23, 103)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(228, 356)\n",
      "INFO:tensorflow:(100, 228)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(94, 205)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(98, 354)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(237, 239)\n",
      "INFO:tensorflow:(109, 111)\n",
      "INFO:tensorflow:(129, 138)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(158, 161)\n",
      "INFO:tensorflow:(30, 33)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(300, 327)\n",
      "INFO:tensorflow:(172, 199)\n",
      "INFO:tensorflow:(44, 71)\n",
      "INFO:tensorflow:(267, 268)\n",
      "INFO:tensorflow:(139, 140)\n",
      "INFO:tensorflow:(11, 12)\n",
      "INFO:tensorflow:(151, 273)\n",
      "INFO:tensorflow:(23, 145)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(191, 364)\n",
      "INFO:tensorflow:(63, 236)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(50, 55)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(286, 289)\n",
      "INFO:tensorflow:(158, 161)\n",
      "INFO:tensorflow:(30, 33)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(174, 316)\n",
      "INFO:tensorflow:(46, 188)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(133, 235)\n",
      "INFO:tensorflow:(121, 122)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(262, 262)\n",
      "INFO:tensorflow:(134, 134)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(150, 263)\n",
      "INFO:tensorflow:(22, 135)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(123, 124)\n",
      "INFO:tensorflow:(316, 316)\n",
      "INFO:tensorflow:(188, 188)\n",
      "INFO:tensorflow:(60, 60)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(323, 325)\n",
      "INFO:tensorflow:(195, 197)\n",
      "INFO:tensorflow:(67, 69)\n",
      "INFO:tensorflow:(181, 183)\n",
      "INFO:tensorflow:(53, 55)\n",
      "INFO:tensorflow:(215, 353)\n",
      "INFO:tensorflow:(87, 225)\n",
      "INFO:tensorflow:(134, 136)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(288, 288)\n",
      "INFO:tensorflow:(160, 160)\n",
      "INFO:tensorflow:(32, 32)\n",
      "INFO:tensorflow:(138, 139)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(229, 243)\n",
      "INFO:tensorflow:(101, 115)\n",
      "INFO:tensorflow:(19, 292)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(263, 264)\n",
      "INFO:tensorflow:(135, 136)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(94, 354)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(227, 342)\n",
      "INFO:tensorflow:(99, 214)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(322, 325)\n",
      "INFO:tensorflow:(194, 197)\n",
      "INFO:tensorflow:(66, 69)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(346, 347)\n",
      "INFO:tensorflow:(218, 219)\n",
      "INFO:tensorflow:(90, 91)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(212, 357)\n",
      "INFO:tensorflow:(84, 229)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(236, 258)\n",
      "INFO:tensorflow:(108, 130)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(249, 253)\n",
      "INFO:tensorflow:(121, 125)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(145, 300)\n",
      "INFO:tensorflow:(17, 172)\n",
      "INFO:tensorflow:(137, 320)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(150, 152)\n",
      "INFO:tensorflow:(22, 24)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(267, 270)\n",
      "INFO:tensorflow:(139, 142)\n",
      "INFO:tensorflow:(11, 14)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(254, 258)\n",
      "INFO:tensorflow:(126, 130)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(379, 380)\n",
      "INFO:tensorflow:(251, 252)\n",
      "INFO:tensorflow:(123, 124)\n",
      "INFO:tensorflow:(80, 81)\n",
      "INFO:tensorflow:(286, 296)\n",
      "INFO:tensorflow:(158, 168)\n",
      "INFO:tensorflow:(30, 40)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(172, 311)\n",
      "INFO:tensorflow:(44, 183)\n",
      "INFO:tensorflow:(372, 377)\n",
      "INFO:tensorflow:(244, 249)\n",
      "INFO:tensorflow:(116, 121)\n",
      "INFO:tensorflow:(0, 0)\n",
      "INFO:tensorflow:(0, 0)\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "_train_file = '/Users/deniz/natural_questions/data/v1.0_sample_nq-train-sample.jsonl'\n",
    "with jsonlines.open(_train_file) as reader:\n",
    "    features, examples = [], []\n",
    "    for i, example in enumerate(reader):\n",
    "        if i % 1e3 == 0: tf.logging.info(\"{}:{}\".format(_train_file, i))\n",
    "        examples.append(example)\n",
    "        dt = convert_example(example,\n",
    "                             features,\n",
    "                            tokenizer=token,\n",
    "                            is_training=True,\n",
    "                            max_seq_length=384,\n",
    "                        doc_stride=128,\n",
    "                        max_query_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 338)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples), len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2975172535563055798"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = features[10]\n",
    "feature.example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(i):\n",
    "    feature = features[i]\n",
    "    example_id = feature.example_id\n",
    "    example = [x for x in examples if x['example_id'] == example_id][0]\n",
    "    return feature, example  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotations(example):\n",
    "    \"\"\"\n",
    "    if short, else long\n",
    "    \"\"\"\n",
    "    annotation = example['annotations'][0]\n",
    "    end_byte_ix, start_byte_ix = None, None\n",
    "    start_token, end_token = None, None\n",
    "    if annotation['short_answers']:\n",
    "        end_byte_ix = annotation['short_answers'][0]['end_byte']\n",
    "        start_token = annotation['short_answers'][0]['start_token']\n",
    "        end_token = annotation['short_answers'][0]['end_token']\n",
    "        start_byte_ix = annotation['short_answers'][0]['start_byte']\n",
    "    else:\n",
    "        end_byte_ix = annotation['long_answer']['end_byte']\n",
    "        start_byte_ix = annotation['long_answer']['start_byte']\n",
    "        start_token = annotation['long_answer']['start_token']\n",
    "        end_token = annotation['long_answer']['end_token']\n",
    "    return {'end_byte_ix': end_byte_ix, \n",
    "            'start_byte_ix': start_byte_ix,\n",
    "            'start_token': start_token,\n",
    "            'end_token': end_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate(i):\n",
    "    # get the feature and example the feature is derived from.\n",
    "    feature, example = test(i)\n",
    "    # get the ground truth annotations.\n",
    "    gt = get_annotations(example)\n",
    "    # get start byte and end bytes for targets.\n",
    "    if feature.targets[0] == 0:\n",
    "        return (i, True)\n",
    "    start_bytes = feature.start_bytes[feature.targets[0]]\n",
    "    end_bytes = feature.end_bytes[feature.targets[1]]\n",
    "    if start_bytes == gt['start_byte_ix'] and end_bytes == gt['end_byte_ix']:\n",
    "        return (i,True)\n",
    "    else:\n",
    "        return (i, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'end_byte_ix': 96731, 'start_byte_ix': 96715, 'start_token': 3521, 'end_token': 3525}\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, True)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature, example = test(1)\n",
    "gt = get_annotations(example)\n",
    "print(gt)\n",
    "start_bytes = feature.start_bytes[feature.targets[0]]\n",
    "end_bytes = feature.end_bytes[feature.targets[1]]\n",
    "feature.targets, start_bytes, end_bytes\n",
    "_validate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = []\n",
    "for i in range(len(features)):\n",
    "    _assertion =  _validate(i)\n",
    "    if not _assertion[1]:\n",
    "        ix.append(i)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth\n",
      "{'end_byte_ix': 55798, 'start_byte_ix': 55137, 'start_token': 893, 'end_token': 1001}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55140, 55794)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assertion fails\n",
    "feature, example = test(ix[2])\n",
    "gt = get_annotations(example)\n",
    "print('ground truth')\n",
    "print(gt)\n",
    "start_bytes = feature.start_bytes[feature.targets[0]]\n",
    "end_bytes = feature.end_bytes[feature.targets[1]]\n",
    "start_bytes, end_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'annotation_id': 13306123758205215060,\n",
       "  'long_answer': {'candidate_index': 32,\n",
       "   'end_byte': 55798,\n",
       "   'end_token': 1001,\n",
       "   'start_byte': 55137,\n",
       "   'start_token': 893},\n",
       "  'short_answers': [],\n",
       "  'yes_no_answer': 'NONE'}]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'end_byte': 55140, 'html_token': True, 'start_byte': 55137, 'token': '<P>'},\n",
       " {'end_byte': 55143, 'html_token': False, 'start_byte': 55140, 'token': 'The'},\n",
       " {'end_byte': 55153,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55144,\n",
       "  'token': 'marooning'},\n",
       " {'end_byte': 55156, 'html_token': False, 'start_byte': 55154, 'token': 'of'},\n",
       " {'end_byte': 55167,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55160,\n",
       "  'token': 'Voyager'},\n",
       " {'end_byte': 55174, 'html_token': False, 'start_byte': 55172, 'token': 'in'},\n",
       " {'end_byte': 55178, 'html_token': False, 'start_byte': 55175, 'token': 'the'},\n",
       " {'end_byte': 55184,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55179,\n",
       "  'token': 'Delta'},\n",
       " {'end_byte': 55193,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55185,\n",
       "  'token': 'Quadrant'},\n",
       " {'end_byte': 55202,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55194,\n",
       "  'token': 'provided'},\n",
       " {'end_byte': 55208,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55203,\n",
       "  'token': 'Paris'},\n",
       " {'end_byte': 55213,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55209,\n",
       "  'token': 'with'},\n",
       " {'end_byte': 55215, 'html_token': False, 'start_byte': 55214, 'token': 'a'},\n",
       " {'end_byte': 55219, 'html_token': False, 'start_byte': 55216, 'token': 'new'},\n",
       " {'end_byte': 55229,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55220,\n",
       "  'token': 'beginning'},\n",
       " {'end_byte': 55230, 'html_token': False, 'start_byte': 55229, 'token': '.'},\n",
       " {'end_byte': 55238,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55231,\n",
       "  'token': 'Janeway'},\n",
       " {'end_byte': 55243,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55239,\n",
       "  'token': 'gave'},\n",
       " {'end_byte': 55249,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55244,\n",
       "  'token': 'Paris'},\n",
       " {'end_byte': 55251, 'html_token': False, 'start_byte': 55250, 'token': 'a'},\n",
       " {'end_byte': 55257,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55252,\n",
       "  'token': 'field'},\n",
       " {'end_byte': 55268,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55258,\n",
       "  'token': 'commission'},\n",
       " {'end_byte': 55271, 'html_token': False, 'start_byte': 55269, 'token': 'as'},\n",
       " {'end_byte': 55273, 'html_token': False, 'start_byte': 55272, 'token': 'a'},\n",
       " {'end_byte': 55283,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55274,\n",
       "  'token': 'Starfleet'},\n",
       " {'end_byte': 55294,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55284,\n",
       "  'token': 'lieutenant'},\n",
       " {'end_byte': 55298, 'html_token': False, 'start_byte': 55295, 'token': 'and'},\n",
       " {'end_byte': 55303,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55299,\n",
       "  'token': 'made'},\n",
       " {'end_byte': 55307, 'html_token': False, 'start_byte': 55304, 'token': 'him'},\n",
       " {'end_byte': 55313,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55308,\n",
       "  'token': 'chief'},\n",
       " {'end_byte': 55322,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55314,\n",
       "  'token': 'helmsman'},\n",
       " {'end_byte': 55325, 'html_token': False, 'start_byte': 55323, 'token': 'of'},\n",
       " {'end_byte': 55336,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55329,\n",
       "  'token': 'Voyager'},\n",
       " {'end_byte': 55341, 'html_token': False, 'start_byte': 55340, 'token': '.'},\n",
       " {'end_byte': 55344, 'html_token': False, 'start_byte': 55342, 'token': 'He'},\n",
       " {'end_byte': 55348, 'html_token': False, 'start_byte': 55345, 'token': 'had'},\n",
       " {'end_byte': 55350, 'html_token': False, 'start_byte': 55349, 'token': 'a'},\n",
       " {'end_byte': 55356,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55351,\n",
       "  'token': 'rough'},\n",
       " {'end_byte': 55362,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55357,\n",
       "  'token': 'start'},\n",
       " {'end_byte': 55363, 'html_token': False, 'start_byte': 55362, 'token': ','},\n",
       " {'end_byte': 55371,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55364,\n",
       "  'token': 'however'},\n",
       " {'end_byte': 55372, 'html_token': False, 'start_byte': 55371, 'token': ','},\n",
       " {'end_byte': 55375, 'html_token': False, 'start_byte': 55373, 'token': 'as'},\n",
       " {'end_byte': 55385,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55376,\n",
       "  'token': 'Starfleet'},\n",
       " {'end_byte': 55389, 'html_token': False, 'start_byte': 55386, 'token': 'and'},\n",
       " {'end_byte': 55396,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55390,\n",
       "  'token': 'Maquis'},\n",
       " {'end_byte': 55402,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55397,\n",
       "  'token': 'alike'},\n",
       " {'end_byte': 55409,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55403,\n",
       "  'token': 'viewed'},\n",
       " {'end_byte': 55415,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55410,\n",
       "  'token': 'Paris'},\n",
       " {'end_byte': 55420,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55416,\n",
       "  'token': 'with'},\n",
       " {'end_byte': 55430,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55421,\n",
       "  'token': 'suspicion'},\n",
       " {'end_byte': 55431, 'html_token': False, 'start_byte': 55430, 'token': '.'},\n",
       " {'end_byte': 55437,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55432,\n",
       "  'token': 'Paris'},\n",
       " {'end_byte': 55444,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55438,\n",
       "  'token': 'worked'},\n",
       " {'end_byte': 55449,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55445,\n",
       "  'token': 'hard'},\n",
       " {'end_byte': 55452, 'html_token': False, 'start_byte': 55450, 'token': 'to'},\n",
       " {'end_byte': 55457,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55453,\n",
       "  'token': 'earn'},\n",
       " {'end_byte': 55461, 'html_token': False, 'start_byte': 55458, 'token': 'his'},\n",
       " {'end_byte': 55471,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55462,\n",
       "  'token': 'crewmates'},\n",
       " {'end_byte': 55476, 'html_token': False, 'start_byte': 55471, 'token': \"'\"},\n",
       " {'end_byte': 55484,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55477,\n",
       "  'token': 'respect'},\n",
       " {'end_byte': 55485, 'html_token': False, 'start_byte': 55484, 'token': '.'},\n",
       " {'end_byte': 55492,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55486,\n",
       "  'token': 'During'},\n",
       " {'end_byte': 55497,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55493,\n",
       "  'token': 'this'},\n",
       " {'end_byte': 55502,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55498,\n",
       "  'token': 'time'},\n",
       " {'end_byte': 55503, 'html_token': False, 'start_byte': 55502, 'token': ','},\n",
       " {'end_byte': 55506, 'html_token': False, 'start_byte': 55504, 'token': 'he'},\n",
       " {'end_byte': 55513,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55507,\n",
       "  'token': 'became'},\n",
       " {'end_byte': 55518,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55514,\n",
       "  'token': 'best'},\n",
       " {'end_byte': 55526,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55519,\n",
       "  'token': 'friends'},\n",
       " {'end_byte': 55531,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55527,\n",
       "  'token': 'with'},\n",
       " {'end_byte': 55538,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55532,\n",
       "  'token': 'Ensign'},\n",
       " {'end_byte': 55612,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55607,\n",
       "  'token': 'Harry'},\n",
       " {'end_byte': 55616, 'html_token': False, 'start_byte': 55613, 'token': 'Kim'},\n",
       " {'end_byte': 55621, 'html_token': False, 'start_byte': 55620, 'token': ','},\n",
       " {'end_byte': 55623, 'html_token': False, 'start_byte': 55622, 'token': 'a'},\n",
       " {'end_byte': 55629,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55624,\n",
       "  'token': 'young'},\n",
       " {'end_byte': 55637,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55630,\n",
       "  'token': 'officer'},\n",
       " {'end_byte': 55640, 'html_token': False, 'start_byte': 55638, 'token': 'on'},\n",
       " {'end_byte': 55644, 'html_token': False, 'start_byte': 55641, 'token': 'his'},\n",
       " {'end_byte': 55650,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55645,\n",
       "  'token': 'first'},\n",
       " {'end_byte': 55658,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55651,\n",
       "  'token': 'mission'},\n",
       " {'end_byte': 55662, 'html_token': False, 'start_byte': 55659, 'token': 'who'},\n",
       " {'end_byte': 55669,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55663,\n",
       "  'token': 'defied'},\n",
       " {'end_byte': 55673, 'html_token': False, 'start_byte': 55670, 'token': 'his'},\n",
       " {'end_byte': 55683,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55674,\n",
       "  'token': 'crewmates'},\n",
       " {'end_byte': 55686, 'html_token': False, 'start_byte': 55684, 'token': 'to'},\n",
       " {'end_byte': 55695,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55687,\n",
       "  'token': 'befriend'},\n",
       " {'end_byte': 55701,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55696,\n",
       "  'token': 'Paris'},\n",
       " {'end_byte': 55702, 'html_token': False, 'start_byte': 55701, 'token': '.'},\n",
       " {'end_byte': 55713,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55703,\n",
       "  'token': 'Eventually'},\n",
       " {'end_byte': 55714, 'html_token': False, 'start_byte': 55713, 'token': ','},\n",
       " {'end_byte': 55720,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55715,\n",
       "  'token': 'Paris'},\n",
       " {'end_byte': 55724, 'html_token': False, 'start_byte': 55721, 'token': 'was'},\n",
       " {'end_byte': 55733,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55725,\n",
       "  'token': 'accepted'},\n",
       " {'end_byte': 55736, 'html_token': False, 'start_byte': 55734, 'token': 'by'},\n",
       " {'end_byte': 55740, 'html_token': False, 'start_byte': 55737, 'token': 'the'},\n",
       " {'end_byte': 55745,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55741,\n",
       "  'token': 'crew'},\n",
       " {'end_byte': 55749, 'html_token': False, 'start_byte': 55746, 'token': 'and'},\n",
       " {'end_byte': 55756,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55750,\n",
       "  'token': 'became'},\n",
       " {'end_byte': 55760, 'html_token': False, 'start_byte': 55757, 'token': 'one'},\n",
       " {'end_byte': 55763, 'html_token': False, 'start_byte': 55761, 'token': 'of'},\n",
       " {'end_byte': 55771,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55764,\n",
       "  'token': 'Janeway'},\n",
       " {'end_byte': 55777, 'html_token': False, 'start_byte': 55771, 'token': \"'s\"},\n",
       " {'end_byte': 55784,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55778,\n",
       "  'token': 'valued'},\n",
       " {'end_byte': 55793,\n",
       "  'html_token': False,\n",
       "  'start_byte': 55785,\n",
       "  'token': 'officers'},\n",
       " {'end_byte': 55794, 'html_token': False, 'start_byte': 55793, 'token': '.'},\n",
       " {'end_byte': 55798, 'html_token': True, 'start_byte': 55794, 'token': '</P>'}]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = get_annotations(example)\n",
    "[t for t in example['document_tokens'] if t['start_byte'] >= ann['start_byte_ix'] and t['end_byte'] <= ann['end_byte_ix']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_nq import input_fn_builder\n",
    "seq_length=512\n",
    "\n",
    "name_to_features = {\n",
    "  \"input_ids\": tf.FixedLenFeature([], tf.int64),\n",
    "  \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "  \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "  \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "  \"start_bytes\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "  \"end_bytes\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "}\n",
    "name_to_features[\"start_positions\"] = tf.FixedLenFeature([], tf.int64)\n",
    "name_to_features[\"end_positions\"] = tf.FixedLenFeature([], tf.int64)\n",
    "\n",
    "def _decode_record(record):\n",
    "  \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "  example = tf.parse_single_example(record, name_to_features)\n",
    "  return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import run_nq\n",
    "#bert_data_dir = '/Users/deniz/natural_questions/data/'\n",
    "_train_path = '/Users/deniz/natural_questions/data/v1.0_sample_nq-train-sample.jsonl'\n",
    "_dev_path = os.path.join(bert_data_dir, 'dev')\n",
    "_train_path = os.path.join(bert_data_dir, 'train')\n",
    "train_files = [os.path.join(_train_path, _file) for _file in os.listdir(_train_path) if _file.endswith(\".tf_record\")]\n",
    "\n",
    "train_input_fn = input_fn_builder(\n",
    "  input_files=train_files,\n",
    "  seq_length=512,\n",
    "  is_training=True,\n",
    "  mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "dt = tf.data.TFRecordDataset(train_files)\n",
    "dt = dt.map(_decode_record, num_parallel_calls=10)\n",
    "dt = dt.shuffle(buffer_size=100)\n",
    "dt = dt.batch(32)\n",
    "it = dt.make_one_shot_iterator()\n",
    "a = it.get_next()\n",
    "a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context\n",
    "a = tf.constant([[2,10],[3,20],[4,50]])\n",
    "a = tf.constant([[0.2,0.5,0.3]])\n",
    "a = tf.expand_dims(a,1)\n",
    "print(a.shape)\n",
    "b = tf.constant([[1,2],[10,20],[1,2]])\n",
    "b = tf.constant([[1,2]])\n",
    "b = tf.expand_dims(b,2)\n",
    "print(b.shape)\n",
    "c = a + b\n",
    "out = c.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[0.2, 0.5, 0.3]])\n",
    "b = tf.constant([[0.8, 0.1, 0.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.matmul(a,b, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_a = tf.expand_dims(a,0)\n",
    "_b = tf.expand_dims(b,-1)\n",
    "c = _a * _b\n",
    "d = c.numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices = tf.argmax(c, axis=1)  # this gives you indices from 0 to 600^2\n",
    "col_indices = indices / 3\n",
    "row_indices = indices % 3\n",
    "final_indices = tf.transpose(tf.stack(col_indices, row_indices))\n",
    "final_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.argmax(d,axis=[0,1])\n",
    "d.argmax(axis=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax_2d(tensor):\n",
    "\n",
    "  # input format: BxHxWxD\n",
    "  assert rank(tensor) == 4\n",
    "\n",
    "  # flatten the Tensor along the height and width axes\n",
    "  flat_tensor = tf.reshape(tensor, (tf.shape(tensor)[0], -1, tf.shape(tensor)[3]))\n",
    "\n",
    "  # argmax of the flat tensor\n",
    "  argmax = tf.cast(tf.argmax(flat_tensor, axis=1), tf.int32)\n",
    "\n",
    "  # convert indexes into 2D coordinates\n",
    "  argmax_x = argmax // tf.shape(tensor)[2]\n",
    "  argmax_y = argmax % tf.shape(tensor)[2]\n",
    "\n",
    "  # stack and return 2D coordinates\n",
    "  return tf.stack((argmax_x, argmax_y), axis=1)\n",
    "\n",
    "def rank(tensor):\n",
    "\n",
    "  # return the rank of a Tensor\n",
    "  return len(tensor.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax_2d(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gather(tensor, indices):\n",
    "  \"\"\"Gather in batch from a tensor of arbitrary size.\n",
    "\n",
    "  In pseudocode this module will produce the following:\n",
    "  output[i] = tf.gather(tensor[i], indices[i])\n",
    "\n",
    "  Args:\n",
    "    tensor: Tensor of arbitrary size.\n",
    "    indices: Vector of indices.\n",
    "  Returns:\n",
    "    output: A tensor of gathered values.\n",
    "  \"\"\"\n",
    "  shape = get_shape(tensor)\n",
    "  flat_first = tf.reshape(tensor, [shape[0] * shape[1]] + shape[2:])\n",
    "  indices = tf.convert_to_tensor(indices)\n",
    "  offset_shape = [shape[0]] + [1] * (indices.shape.ndims - 1)\n",
    "  offset = tf.reshape(tf.range(shape[0]) * shape[1], offset_shape)\n",
    "  output = tf.gather(flat_first, indices + offset)\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#####accuracy metric#####\n",
    "#########################\n",
    "\n",
    "tf.reset_default_graph()\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import metrics\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    start_ix = tf.expand_dims(tf.constant([10,20,30,40,50]),1)\n",
    "    end_ix =  tf.expand_dims(tf.constant([10,20,30,40,50]),1)\n",
    "    start_positions = tf.expand_dims(tf.constant([10,20,30,40,50]),1)\n",
    "    end_positions = tf.expand_dims(tf.constant([10,20,30,40,60]),1) #80% accuracy\n",
    "\n",
    "    y_pred = tf.concat([start_ix, end_ix], axis=-1) #[batch_size, 2]\n",
    "    y_true = tf.concat([start_positions, end_positions], axis=-1) #[batch_size, 2]\n",
    "    acc = tf.reduce_all(math_ops.equal(y_true, y_pred), axis=-1)\n",
    "    is_correct = math_ops.to_float(acc)\n",
    "    a,b = metrics.mean(is_correct)\n",
    "    \n",
    "    \n",
    "    running_vars = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES)\n",
    "    running_vars_initializer = tf.variables_initializer(var_list=running_vars)\n",
    "    \n",
    "    sess.run(running_vars_initializer)\n",
    "    \n",
    "    # initial op\n",
    "    a_out = sess.run(a)\n",
    "    # update op\n",
    "    b_out = sess.run(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_out, b_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from preprocessing.preprocessing import *\n",
    "from run_nq import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_data_dir = \"/data/nq/natural_questions/v1.0/sample_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_files = [_file for _file in os.listdir(bert_data_dir) if _file.endswith(\".tf_record\")]\n",
    "_file_path = [os.path.join(bert_data_dir, _file) for _file in train_files]\n",
    "[tf.gfile.MakeDirs(_dir) for _dir in [_train_path, _dev_path]]\n",
    "print(_file_path)\n",
    "train_input_fn = input_fn_builder(\n",
    "    input_file=_file_path,\n",
    "    seq_length=512,\n",
    "    is_training=True,\n",
    "    drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['batch_size'] = 32\n",
    "_iter = train_input_fn(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_iter = _iter.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = _iter.get_next()\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt['document_tokens'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = convert_examples_to_features(dt, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer\n",
    "if outputs:\n",
    "    print(outputs[0].targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{i: t for i, t in enumerate(outputs[0].tokens) if i >= outputs[0].targets[0][0] and i <= outputs[0].targets[0][1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_answer_start = dt['annotations'][0]['short_answers'][0]['start_byte']\n",
    "short_answer_end = dt['annotations'][0]['short_answers'][0]['end_byte']\n",
    "[t for t in dt['document_tokens'] if t['start_byte'] >= short_answer_start and t['end_byte'] <= short_answer_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_answer_start = dt['annotations'][0]['long_answer']['start_byte']\n",
    "long_answer_end = dt['annotations'][0]['long_answer']['end_byte']\n",
    "end = dt['annotations'][0]['long_answer']['end_token']+1\n",
    "start = dt['annotations'][0]['long_answer']['start_token']\n",
    "dt['document_tokens'][start:end]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Wordpiece\n",
    "1 - wordpiece the 'document_tokens' (remove html?)\n",
    "2 - rearrrange long and short answer byte indices accordingly.\n",
    "3 - above logic re-produces the entire text, which is too large. need to do\n",
    "    ([CLS], tokenized question, [SEP], token from content, [SEP].) -> 512 dimensions\n",
    "    \n",
    "the paper only collects short spans but falls back to long span if short span is not available. \n",
    "take examples where both short **and** long exists\n",
    "(35% of the dataset. another 37% where long answer is not available but good question.)\n",
    "\n",
    "------------------\n",
    "for each doc:\n",
    "    wordpiece the question. determine the span. this is dimensions minus the question minus special chars.\n",
    "    wordpiece the 'document_tokens' (remove html?)\n",
    "    rearrrange long and short answer byte indices accordingly.\n",
    "    \n",
    "    for each span:\n",
    "        slide the window. determine if short answer / long answer (?) is contained in the span.\n",
    "        could be multiple short answers.\n",
    "        adjust the ix accordingly. if not mark is as null / [CLS]?\n",
    "    downsample null instances\n",
    "    dump json\n",
    "    \n",
    "-------------------\n",
    "how do you train on both long and short answers? the paper emits short answers and relies on DOM to extract long answers. \n",
    "In addition, at inference time, the authors change the answers to **long only** or **no answers** based on QA eval script. \n",
    "\n",
    "long answer must be within a HTML bounding box (a paragraph or a table).\\\n",
    "\n",
    "document-QA, which is a baseline, makes a prediction but whether the prediction is long or short is determined by the inclusion of start / end of passage tokens.\n",
    "\n",
    "whether long answer exists is determined by the threshold.\n",
    "or in document-qa. predict. if the span goes over a passage, that is the long answers. and short answer does not exist?\n",
    "\n",
    "a single answer to predict on. if short answer, use. if long answer, use thsat. \n",
    "\n",
    "long answer only: 13%\n",
    "long and short: 35%\n",
    "\n",
    "\n",
    "--------------------\n",
    "how do i do it multithreaded? dask\n",
    "--------------------\n",
    "BERT: question has a positional embedding A, and answer has B.\n",
    "\n",
    "--------------------\n",
    "need to prredict if the answer is NULL. done through some post-processing heuristics on eval script score cutoff?\n",
    "\n",
    "--------------------\n",
    "what is a score.\n",
    "\n",
    "--------------------\n",
    "preprocessing should take something like a dict of labels and map them onto the new. this will apply to both problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['document_tokens'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set([t['token'] for dt in data for t in dt['document_tokens'] if t['html_token']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(dt['document_html']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt['long_answer_candidates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['question_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['annotations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = dt['document_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token for token in tokens if token['start_byte'] >= start_byte_ix and token['end_byte'] <= end_byte_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' '.join([token['token'] for token in tokens if token['start_byte'] >= start_byte_ix_long and token['end_byte'] <= end_byte_ix_long and not token['html_token']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3tf",
   "language": "python",
   "name": "py3tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
